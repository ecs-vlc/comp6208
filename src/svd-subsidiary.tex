% Created 2024-02-09 Fri 16:25
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[a4paper,margin=20mm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{bm}
\usepackage{minted}
\usemintedstyle{emacs}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beraserif}
\usepackage[scaled]{berasans}
\usepackage[scaled]{beramono}
\newcommand{\tr}{\textsf{T}}
\newcommand{\grad}{\bm{\nabla}}
\newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
\newcommand{\Prob}[2][]{\mathbb{P}_{#1\!}\left[ #2 \right]}
\newcommand{\logg}[1]{\log\!\left( #1 \right)}
\newcommand{\pred}[1]{\left\llbracket { \small #1} \right\rrbracket}
\newcommand{\e}[1]{{\rm e}^{#1}}
\newcommand{\dd}{\mathrm{d}}
\DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
\newcommand{\normal}[2]{\mathcal{N}\!\left(#1 \big| #2 \right)}
\newcounter{eqCounter}
\setcounter{eqCounter}{0}
\newcommand{\explanation}{\setcounter{eqCounter}{0}\renewcommand{\labelenumi}{(\arabic{enumi})}}
\newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\arabic{eqCounter})}}{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\Dist}[2][Binom]{\mathrm{#1}\left( \strut {#2} \right)}
\author{Adam Prügel-Bennett}
\date{\today}
\title{Advanced Machine Learning Subsidary Notes\\\medskip
\large Lecture 12: Singular Value Decomposition (SVD)}
\hypersetup{
 pdfauthor={Adam Prügel-Bennett},
 pdftitle={Advanced Machine Learning Subsidary Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.3)}, 
 pdflang={English}}
\begin{document}

\maketitle


\section{Keywords}
\label{sec:org884d725}
\begin{itemize}
\item Singular Valued Decomposition, SVD, general linear maps
\end{itemize}

\section{Main Points}
\label{sec:orgd0fb5f5}

\subsection{Singular Value Decomposition}
\label{sec:orgc1944e5}
\begin{itemize}
\item Any \(n\times m\) matrix, \(\mat{X}\) can be decomposed as \(\mat{X} = \mat{U}\,\mat{S}\,\mat{V}^\tr\)
\begin{itemize}
\item \(\mat{U}\) is an \(n\times n\) orthogonal matrix
\item \(\mat{S}\) is an \(n\times m\)  matrix with zeros everywhere except the diagonal where \(S_{ii}=s_i\geq0\)
\item \(\mat{V}\) is an \(m\times m\) orthogonal matrix
\end{itemize}
\item The values \(s_i\) are known as the \emph{singular values} of \(\mat{X}\)
\item The SVD of a symmetric matrix is just the eigen-decomposition
\item \textbf{Economical SVD}
\begin{itemize}
\item If \(n>m\) some algorithms won't bother outputting the last \(n-m\) columns of \(\mat{U}\)
\item If \(m<m\) some algorithms won't bother outputting the last \(m-n\) columns of \(\mat{V}\)
\item In this case it will output a square \(\mat{S}\) matrix
\end{itemize}
\end{itemize}

\subsection{General Linear Mapping}
\label{sec:org6ff3378}
\begin{itemize}
\item Recall that matrices are the most general linear operators
\item Since any matrix \(\mat{M}\) can be written as
\(\mat{U}\,\mat{S}\,\mat{V}^\tr\) we can interpret any linear
mapping as doing three operations
\begin{itemize}
\item A rotation (with possibly a reflection) defined by \(\mat{V}^\tr\)
\item A rescaling of each coordinate by \(s_i\)
\item A rotation (with possibly a reflection) defined by \(\mat{U}\)
\end{itemize}
\item \textbf{Duality}
\begin{itemize}
\item Using \(\mat{X} = \mat{U}\,\mat{S}\,\mat{V}^\tr\) then
\begin{itemize}
\item \(\mat{C} = \mat{X}\,\mat{X}^\tr = \mat{U}\mat{S}\mat{S}^\tr\mat{U}\)
\item \(\mat{D} = \mat{X}^\tr\mat{X} = \mat{V}\mat{S}^\tr\mat{S}\mat{V}\)
\end{itemize}
\item \(\mat{S}\mat{S}^\tr\) and \(\mat{S}^\tr\mat{S}\) are diagonal
elements with non-zero diagonal elements \(s_i^2\)
\end{itemize}
\end{itemize}

\subsection{Ridge Regression}
\label{sec:org6d7a8b5}
\begin{itemize}
\item Ridge regression is linear regression with an \(L_2\) regulariser
\item Adding a regulariser \(\nu\, \|\bm{w}\|^2\) the weights, \(\bm{w}^*\), that
minimise the loss function are given by \(\bm{w}^* =
     (\mat{X}^\tr\mat{X}+\nu\,\mat{I})^{-1}\mat{X}^\tr \bm{y}\)
\item Using \(\mat{X} = \mat{U}\,\mat{S}\,\mat{V}^\tr\) then
\begin{align*}
\bm{w}^* = \mat{V}\,\bar{\mat{S}}^+ \mat{U}^\tr \bm{y}
\end{align*}
where \(\bar{\mat{S}}^+\) is a regularised pseudo-inverse of \(\mat{S}\) given by
\begin{align*}
\bar{\mat{S}}^+ = (\mat{S}^\tr\mat{S}+\nu\,\mat{I})^{-1}\mat{S}
\end{align*}
\begin{itemize}
\item If \(\nu=0\) this is equal to the pseudo-inverse of \(\mat{S}\)
\end{itemize}
\item \(\bar{\mat{S}}^+\) is and \(n\times m\) matrix which is zero
everywhere except on the diagonal, where \(\bar{S}^+_{ii} = \frac{s_i}{s_i^2+\nu}\)
\begin{itemize}
\item Note if \(s_i=0\) linear regression has an infinity of solutions
and the pseudo-inverse of \(\mat{X}\) does not exist (setting
\(\nu=0\) we get \(S^+_{ii}=1/s_i\) which is not define when \(s_i=0\))
\item In the regularised case \(\bar{S}^+_{ii} = 0\) (we have selected
one of the solutions that minimise the squared error)
\item If \(s_i\ll\nu\) then without the regularisation term the inverse is
very ill-conditions while with the regularisation term
\(\bar{S}^+_{ii}\) will be small
\item If \(s_i\gg\nu\) then \(\bar{S}^+_{ii} \approx \frac{1}{s_i} = S^+_{ii}\)
\end{itemize}
\item Adding a \(L_2\) regulariser means that the optimum weights,
\(\bm{w}^*\), will be less sensitive to the training data reducing
the variance in the bias-variance dilemma
\end{itemize}


\section{Exercises}
\label{sec:orgc645561}

\subsection{Ridge regression}
\label{sec:org377101d}
\begin{itemize}
\item Ridge regression is just linear regression with an \(L_2\) regularier
\begin{enumerate}
\item Derive the optimal weights in ridge regression
\item Show that using \(\mat{X} = \mat{U}\,\mat{S}\,\mat{V}^\tr\) then
\(\bm{w}^* = \mat{V}\,(\mat{S}^\tr\mat{S} +
        \nu\,\mat{I})^{-1}\mat{S}\,\mat{U}\bm{y}\)
\end{enumerate}
\item See answers
\end{itemize}

\section{Experiments}
\label{sec:orgd8d0862}

\subsection{SVD}
\label{sec:orgfb0e5a4}
\begin{itemize}
\item Using Matlab/Octave or python have a play with svd
\end{itemize}
\begin{minted}[]{matlab}
X = randn(3,4)      % construct a random matrix
[U,S,V] = svd(X)    % compute singular value decomposition
U*S*V'              % should be the same as X
U*U'                % should be the identity up to round error
U'*U                % should be the identity up to round error
V*V'                % should be the identity up to round error
V'*V                % should be the identity up to round error
[Ue,L1] = eig(X*X') % Ue should be the same as U up to permutation
S*S'                % same as L1 up to permutation
[Ve,L2] = eig(X'*X) % Ve should be the same as V up to permutation
S'*S                % same as L2 up to permutation


inv(X'*X + 0.1*eye(4))        % check identity
V*inv(S'*S + 0.1*eye(4))*V'   % should be the same
\end{minted}

\subsection{Verify Identity}
\label{sec:orga97e89e}
\begin{itemize}
\item Again use Matlab/Octave or python
\item For a random \(4\times5\) matrix \(\mat{X}\)
\begin{itemize}
\item Check that using \(\mat{X} = \mat{U}\,\mat{S}\,\mat{V}^\tr\) that
\begin{align*}
 (\mat{X}^\tr\mat{X} + \eta\,\mat{I})^{-1} 
  = \mat{V}\,(\mat{S}^\tr\mat{S}+\nu\,\mat{I})^{-1}\mat{V}^\tr
\end{align*}
holds for some random matrix using Matlab/Octave or python
\item Examine \(\mat{S}^\tr\mat{S}\), \(\mat{S}^\tr\mat{S}+0.1\,\mat{I}\).
\((\mat{S}^\tr\mat{S}+0.1\,\mat{I})^{-1}\) and
\((\mat{S}^\tr\mat{S}+0.1\,\mat{I})^{-1} \mat{S}^\tr\)
\item See if you can invert \(\mat{X}^\tr\mat{X}\): it is singular,
but due to rounding errors it might be inverted (it was a scary
matrix when I tried it)
\end{itemize}
\end{itemize}

\begin{minted}[]{matlab}
X = randn(4,5)                % construct a random matrix
[U,S,V] = svd(X)              % compute singular value decomposition

inv(X'*X + 0.1*eye(5))        % check identity
V*inv(S'*S + 0.1*eye(5))*V'   % should be the same

S'*S                          % singular
S'*S + 0.1*eye(5)             % now invertible
inv(S'*S + 0.1*eye(5))        
inv(S'*S + 0.1*eye(5))*S'     % 4x5 diagonal matrix

inv(X'*X)                     % shouldn't be able to do this
\end{minted}

\section{Answers}
\label{sec:org80a2817}
\subsection{Ridge regression}
\label{sec:org192e8c6}
\begin{enumerate}
\item It is straightforward to show
$$ \bm{w}^* = (\mat{X}^\tr\,\mat{X}+\nu\,\mat{I})^{-1}
      \mat{X}^{-1} \bm{y} $$
\item The only hard part is to show is that
\begin{align*}
 (\mat{X}^\tr\mat{X} + \nu\,\mat{I})^{-1} 
 = \mat{V}\,(\mat{S}^\tr\mat{S}+\nu\,\mat{I})^{-1}\mat{V}^\tr
 \end{align*}
\begin{itemize}
\item It is easy to show that \(\mat{X}^\tr\mat{X}=\mat{V}\,\mat{S}^\tr\mat{S}\,\mat{V}^\tr\)
\item But we also have \(\mat{I}=\mat{V}\,\mat{V}^\tr\) as \(\mat{V}\) is an orthogonal matrix
\item Thus \(\mat{M} = \mat{X}^\tr\mat{X} + \nu\,\mat{I} =
        \mat{V}\,(\mat{S}^\tr\mat{S}+\nu\mat{I})\mat{V}^\tr =
        \mat{V}\,\mat{W}\,\mat{V}^\tr\) where \(\mat{W}= \mat{S}^\tr\mat{S}+\nu\mat{I}\)
\item But \((\mat{A}\,\mat{B}\,\mat{C})^{-1} =
        \mat{C}^{-1}\mat{B}^{-1}\mat{A}^{-1}\) (which we can verify by
multiplying \(\mat{C}^{-1}\mat{B}^{-1}\mat{A}^{-1}\) on either
the left or right by \(\mat{A}\,\mat{B}\,\mat{C}\))
\item Thus \(\mat{M}^{-1} = (\mat{V}\,\mat{W}\,\mat{V}^\tr)^{-1} =
        (\mat{V})^\tr}^{-1}\,\mat{W}^{-1}\mat{V}^{-1} =
        \mat{V}\,\mat{W}\mat{V}^\tr\) where we use
\(\mat{V}^{-1}=\mat{V}^\tr\) as \(\mat{V}\) is an orthogonal matrix
\end{itemize}
\end{enumerate}
\end{document}
