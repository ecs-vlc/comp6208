#+TITLE: Advanced Machine Learning Subsidary Notes
#+SUBTITLE: Lecture 19: Wasserstein GANs

* Keywords
  * GANs, Wasserstein distance, Duality, WGANs

* Main Points
** Generative Adversarial Networks GANs
   * GANs are generative models
   * They are often used for generating images or text
     - we will consider images just to be concrete but nothing really
       changes if we use text
   * The task of the GAN is to generate images from the same
     distribution as those of a dataset $\mathcal{D}$
   * GANs use two networks
     1. /A generator network/
	- It is fed a random vector $\bm{z}\sim\mathcal{N}(\bm{0}, \mat{I})$
	- They are usually networks made with fully
          connected and deconvolution layers with
          weights $\bm{w}_G$
	- They generate an "image" $\hat{\bm{x}} = G(\bm{z}, \bm{w}_G)$
	- We train the weights to deceive the discriminator network
     2. /A descriminator network/
	- They receive either
	  + a random sample from $\mathcal{D}$ or
	  + an image $\hat{\bm{x}}$ generated by the generator seeded
            with a random vector $\bm{z}$
	- They are trained using backpropagation where the target
          output is
	  + 1 for the real image from $\mathcal{D}$ or
	  + 0 if the input is from the generator
	- They are usually CNN networks
   * The generator weights are also learned by back-propagation
     through both the descriminator and generator networks where the
     target is for the descriminator for output 1
     - the descriminator weights aren't changed
     - this is opposite to the loss for the descriminator
     - it is fed the information about how to fool the descriminator
       (i.e. how to change the elements of $\hat{\bm{x}}$ to maximise
       the output of the descriminator)
     - Hopefully over time the generator produces image more like
       those from $\mathcal{D}$
   * *Problems with GANs*
     - GANs are notoriously hard to train
     - The training of the descriminator and generator can become decoupled
     - For example, the descriminator can become so good that any
       local change of $\hat{\bm{x}}$ doesn't fool the descriminator
       + but this means there is no gradient to direct the learning of
         the generator
     - To overcome this we consider a very different approach

** Wasserstein Distance
   * The Big Picture
     - We consider minimising the distance between the distribution of
       images generated by the generator (that is, the distribution of
       $\hat{\bm{x}} = G(\bm{z}, \bm{w}_G)$ where
       $\bm{z} \sim \mathcal{N}(\bm{0}, \mat{I})$) and the distribution of real
       images (where we consider $\mathcal{D}$ to be a set of samples
       drawn from this distribution)
     - How do we measure distances between probability distributions?
     - One of the most common methods is to use the KL-divergence
       $$ \mathrm{KL}(p\|q) = \int p(\bm{x}) \,
	\logg{\frac{p(\bm{x})}{q(\bm{y})}} \, \dd \bm{x} $$
       + Relatively nice to compute
       + Not a true distance (but that doesn't bother us)
       + Unfortunately it can get very large even when the probability
         distributions are relatively close together
   * *Earth-Moving Distance*
     - An very natural distance measure is the minimum distances you
       have to move the probability mass in one distribution
       $p(\bm{x})$ to make it identical to a second distribution $q(\bm{x})$
     - This is also known as the /Wasserstein/ distance
     - Although conceptually straightforward it is a bit nasty to compute
     - *Optimal Transport Policy*
       - We start from a transport policy $\gamma(\bm{x},\bm{y})$ that
	 tells us how much probability mass (or density) we need to move
	 from probability distribution $p$ at point $\bm{x}$ to
	 probability distribution $q$ at point $\bm{y}$
       - As we start with a distribution $p(\bm{x})$ we need
	 $$ \int \gamma(\bm{x},\bm{y}) \, \dd \bm{y} &= p(\bm{x}) $$
       - As we end with a distribution $q(\bm{x})$ we require
	 $$ \int \gamma(\bm{x},\bm{y}) \, \dd \bm{x} &= q(\bm{y}) $$
       - Note that $\gamma(\bm{x},\bm{y})$ looks like a joint probability distribution
	 + It is non-negative
	 + Integrating over $\bm{x}$ and $\bm{y}$ we get 1
       - We denote the set of probability distributions that satisfy
	 these constraints $\Lambda(p,q)$
       - The cost of a particular transport policy is
	 $$ C(\gamma) = \int\!\! \int d(\bm{x},\bm{y})\,\gamma(\bm{x},\bm{y}) \,
          \dd\bm{x}\,\dd\bm{y}\pause = \av[\gamma]{d(\bm{x},\bm{y})} $$
	  since $\gamma(\bm{x},\bm{y})$ is the amount of probability
	 mass we move and $d(\bm{x},\bm{y})$ is the distance we move it
       - The optimal transport policy is the distribution $\gamma \in
	 \Lambda(p,q)$ with the minimum cost
       - The cost of the optimal transport policy is the Wasserstein
	 distance
	 $$ W(p,q) = \min_{\gamma \in \Lambda(p,q)}
	 \av[\gamma]{d(\bm{x},\bm{y})} $$
       - For high dimensional probability distributions finding the
	 optimal transport policy using this definition is impractical
     - *Linear Programming*
       - Computing the Wasserstein distance is a linear programming problem
       - We want to choose $\gamma(\bm{x},\bm{y})$ to minimise a linear
	 objective function $C(\gamma)$ subject to linear constraints
       - We can write this as a Lagrange problem
         \begin{align*}
          \mathcal{L} = \int d(\bm{x},\bm{y})\, \gamma(\bm{x},\bm{y})
          \, \dd \bm{x}\,\dd \bm{y}
          &- \int \alpha(\bm{x}) \left( \int \gamma(\bm{x},\bm{y})\,\dd \bm{y}
	 - p(\bm{x})\right) \, \dd \bm{x}\\
          &- \int \beta(\bm{y}) \left( \int \gamma(\bm{x},\bm{y}) \, \dd
          \bm{x} -q(\bm{y})\right) \, \dd \bm{y}
          \end{align*}
         subject to $\gamma(\bm{x},\bm{y})\geq 0$
	 - $\alpha(\bm{x})$ and $\beta(\bm{y})$ are Lagrange multiplier functions
	 - This looks strange because we are used to optimise vectors
           in Linear programming but here we optimise functions
	 - We can discretise the function and we would get a vector
	 - But functions form a vector space so  we can define a
           linear programme for functions
       - *Dual Form*
	 - We can rearrange the Lagrangian as
 	    \begin{align*}
             \mathcal{L} = \int \alpha(\bm{x})\,p(\bm{x})\, \dd \bm{x}
	   + \int \beta(\bm{y})\,q(\bm{y})\,\dd \bm{y}
	     - \int \gamma(\bm{x},\bm{y})\left( \alpha(\bm{x}) + \beta(\bm{y}) -
             d(\bm{x},\bm{y})\right)\, \dd \bm{x}\,\dd \bm{y}\pause
           \end{align*}
	 - Now we can interpret $\gamma(\bm{x},\bm{y})$ as a Lagrange
           multiplier function so that the dual problem is
	   $$ \max_{\alpha(\bm{x}),\beta(\bm{x})} \; \int
           \alpha(\bm{x})\,p(\bm{x})\, \dd \bm{x} + \int 
           \beta(\bm{y})\,q(\bm{y})\,\dd \bm{y} $$
	   subject to
	   $$ \alpha(\bm{x}) + \beta(\bm{y}) \leq d(\bm{x},\bm{y}) $$
	   - note this is an inequality constraint because $\gamma(\bm{x},\bm{y})\geq0$
	 - But this has to be true when $\bm{x}=\bm{y}$ so
	   $$ \alpha(\bm{x}) + \beta(\bm{x}) \leq d(\bm{x},\bm{x}) = 0 $$
	 - Thus $\beta(\bm{x}) = -\alpha(\bm{x}) - \epsilon(\bm{x})$
           where $\epsilon(\bm{x})\geq0$
	 - Our objective function becomes
	   $$  \int \alpha(\bm{x})\,p(\bm{x})\, \dd \bm{x} + \int
            \beta(\bm{y})\,q(\bm{y})\,\dd \bm{y}
            = \int \alpha(\bm{x})\,\left(p(\bm{x}) - q(\bm{x})\right) \, \dd
            \bm{x} - \int q(\bm{x}) \, \epsilon(\bm{x}) \,\dd\bm{x} $$
	 - But this is clearly maximised when $\epsilon(\bm{x})=0$
           therefore $\beta(\bm{x}) = -\alpha(\bm{x})$
	 - The problem simplifies to 
	   $$ \max_{\alpha(\bm{x})} \; \int
           \alpha(\bm{x})\,p(\bm{x})\, \dd \bm{x} - \int 
           \alpha(\bm{y})\,q(\bm{y})\,\dd \bm{y} =
           \max_{\alpha(\bm{x})} \left( \av[p]{\alpha(\bm{x})} -
           \av[q]{\alpha(\bm{x})} \strut \right) $$
	   subject to
	   $$ \alpha(\bm{x}) - \alpha(\bm{y}) \leq d(\bm{x},\bm{y}) $$
	 - functions $\alpha(\bm{x})$ that satisfy this constraint are
           known as /Lipschitz-1 functions/
	 - An equivalent condition is that
	   $$ \| \grad_{\bm{x}} \alpha(\bm{x}) \| \leq 1 $$
	   - this is a continuity condition saying the output has to
             change slowly as we change the input

** Wasserstein GANs
   * In our Wasserstein GAN we train a generator to minimise the
     Wasserstein distance between the distribution of images from the
     generator and the true distribution
   * We use mini-batches to approximate the expectations
      \begin{align*}
       \av[p]{\alpha(\bm{x})}
      &\approx \frac{1}{|\mathcal{B}|}
        \sum_{\bm{x}\in\mathcal{B}} \alpha(\bm{x}),
      &
        \av[q]{\alpha(\bm{x})}
      &\approx \frac{1}{n} \sum_{i=1}^n
        \alpha(G(\bm{z}_i,\bm{w}_G))\pause
    \end{align*}
   * We need to find the function $\alpha(\bm{x})$ that maximises the
     difference between these expectations
   * We make $\alpha(\bm{x})$ a neural network called the /critic/
     - this plays the same role as the discriminator in a normal GAN
     - Again we make this a CNN
     - The difference is it has to be Lipschitz-1
     - This is difficult to achieve and is usually bodged (you can
       read the literature if you are interested)
   * Wasserstein GANs claim to solve many of the problems of normal
     GANs
     - They are not perfect because they only approximate the
       Lipschitz-1
   * They are for me one of the elegant solutions in machine learning
     of the last few years



# * Exercises


# * Experiments


* COMMENT [[file:wasserstein.pdf][PDF]] [[file:pdf/wasserstein_prn.pdf][print]]
* COMMENT [[file:kernelTrick-subsidiary.org][Previous]] [[file:probability-subsidiary.org][Next]]
* Options                                                  :ARCHIVE:noexport:
#+BEGIN_OPTIONS
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage[a4paper,margin=20mm]{geometry}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{stmaryrd}
#+LATEX_HEADER: \usepackage{bm}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usemintedstyle{emacs}
#+LaTeX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \usepackage[scaled]{beraserif}
#+LaTeX_HEADER: \usepackage[scaled]{berasans}
#+LaTeX_HEADER: \usepackage[scaled]{beramono}
#+LATEX_HEADER: \newcommand{\tr}{\textsf{T}}
#+LATEX_HEADER: \newcommand{\grad}{\bm{\nabla}}
#+LATEX_HEADER: \newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
#+LATEX_HEADER: \newcommand{\Prob}[2][]{\mathbb{P}_{#1\!}\left[ #2 \right]}
#+LATEX_HEADER: \newcommand{\logg}[1]{\log\!\left( #1 \right)}
#+LATEX_HEADER: \newcommand{\pred}[1]{\left\llbracket { \small #1} \right\rrbracket}
#+LATEX_HEADER: \newcommand{\e}[1]{{\rm e}^{#1}}
#+LATEX_HEADER: \newcommand{\dd}{\mathrm{d}}
#+LATEX_HEADER: \DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
#+LATEX_HEADER: \newcommand{\normal}[2]{\mathcal{N}\!\left(#1 \big| #2 \right)}
#+LATEX_HEADER: \newcounter{eqCounter}
#+LATEX_HEADER: \setcounter{eqCounter}{0}
#+LATEX_HEADER: \newcommand{\explanation}{\setcounter{eqCounter}{0}\renewcommand{\labelenumi}{(\arabic{enumi})}}
#+LATEX_HEADER: \newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\arabic{eqCounter})}}{#1}}
#+LATEX_HEADER: \newcommand{\argmax}{\mathop{\mathrm{argmax}}}
#+LATEX_HEADER: \newcommand{\Dist}[2][Binom]{\mathrm{#1}\left( \strut {#2} \right)}
#+END_OPTIONS

