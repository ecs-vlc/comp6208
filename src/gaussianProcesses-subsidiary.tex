% Created 2024-02-09 Fri 16:20
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[a4paper,margin=20mm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\newcommand{\tr}{\textsf{T}}
\newcommand{\grad}{\bm{\nabla}}
\newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
\newcommand{\logg}[1]{\log\!\left( #1 \right)}
\newcommand{\e}[1]{{\rm e}^{#1}}
\newcommand{\E}[1]{{\rm e}^{\displaystyle #1}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\normal}[1]{\mathcal{N}\!\left( #1 \right)}
\newcounter{eqCounter}
\setcounter{eqCounter}{0}
\newcommand{\reseteq}{\setcounter{eqCounter}{0}}
\newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\alph{eqCounter})}}{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
\newcommand{\vl}{\bm{\ell}}
\newcommand{\vb}{\bm{b}}
\usepackage{minted}
\usemintedstyle{emacs}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beraserif}
\usepackage[scaled]{berasans}
\usepackage[scaled]{beramono}
\author{Adam Prügel-Bennett}
\date{\today}
\title{Advanced Machine Learning Subsidary Notes\\\medskip
\large Lecture 22: Gaussian Processes}
\hypersetup{
 pdfauthor={Adam Prügel-Bennett},
 pdftitle={Advanced Machine Learning Subsidary Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.3)}, 
 pdflang={English}}
\begin{document}

\maketitle


\section{Keywords}
\label{sec:orgf9837f6}
\begin{itemize}
\item Gaussian Processes, regression
\end{itemize}

\section{Main Points}
\label{sec:orga4003bd}

\subsection{The Big Picture}
\label{sec:orgbb44246}
\begin{itemize}
\item Gaussian processes are a machine learning method for regression
\begin{itemize}
\item You can use them for classification but it is regression where they excel
\end{itemize}
\item They put a prior on functions (preferring smooth functions)
\item Conceptually they are easy
\item They are easy to use
\item But they are mathematically fiddly to understand
\item Gaussian processes were first used for spatial modelling where
the technique was know as \emph{krigging}
\item They are one of the most powerful methods we have of doing regression
\item They are quite general purpose as the prior we use is reasonable
for a large number of applications
\end{itemize}

\subsection{Gaussian Processes}
\label{sec:org66bce1c}
\begin{itemize}
\item Gaussian Processes are also mathematical objects
\item A Gaussian Process assigns a  probability to functions
depending on their spatial smoothness
\begin{itemize}
\item We assume that the probability of \(f(\bm{x})\) taking a
particular value is normally/Gaussian distributed in a way that
depends on the value of  value of the function at other points
\end{itemize}
\item This will be governed by a \emph{covariance} or \emph{kernel} function
\(k(\bm{x},\bm{y})\)  which tells us the covariance of the prior
$$ \av[f]{ (f(\bm{x})-m(\bm{x})) \, (f(\bm{y})-m(\bm{y}))\strut } =
     k(\bm{x},\bm{y}) $$
\begin{itemize}
\item This is an expectation over all functions from our probability distribution
\item \(m(\bm{x})\) encodes a prior belief in the value of
\(f(\bm{x})\)---normally we choose this to be 0 as most often
we don't have any prior knowledge of the function
\item Because the covariance is quadratic it will be positive semi-definite
\item The kernel function is critical to determining the type of
function we will learn
\item Choosing this kernel function correctly is essential to make GP work
\end{itemize}
\item The values at each point \(\bm{x}\) is normally or Gaussianly
distributed
$$ p(f|m,k) \propto \E{-\tfrac{1}{2} \int (f(\bm{x})-m(\bm{x})) 
      \, k^{-1}(\bm{x},\bm{y}) \, (f(\bm{y}) -m(\bm{y})) \, \dd \bm{x}
      \, \dd \bm{y}} $$
\begin{itemize}
\item It is a little unclear exactly what this means for functions: working
with function spaces one has to be very careful (e.g. to
normalise this we have to sum over all functions)
\item Nevertheless it is clear that we can use this for comparing the
likelihood of different functions
\item If you discretise the domain of \(\bm{x}\) then
$$ p(f|m,k) \propto \E{-\tfrac{1}{2} \sum_{i,j} (f(\bm{x}_i)-m(\bm{x}_i)) 
        \, k^{-1}(\bm{x}_i,\bm{x}_j) \, (f(\bm{x}_j) -m(\bm{x}_j)) } $$
\begin{itemize}
\item the probability function is now a well-define  multi-dimensional
normal distribution
\item here it is clear that the value at each point is normally
distributed
\item to get back to the integral form we have to take a limit where our
discrete points become every closer together (this stresses out
mathematicians who point out that nasty things can happen, but
for the rest of us we just go ahead)
\end{itemize}
\item We can sample Gaussian processes from this distribution
\begin{itemize}
\item in low dimensions this allows us to visualise the type of
functions we are likely to learn
\item you can try this out in the experiment section
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Bayesian Inference}
\label{sec:orgc8008c7}
\begin{itemize}
\item We can use Gaussian Processes as a prior for performing regression
\item We assume we have a set of data points \(\mathcal{D} = \left( (\bm{x}_i, y_i)
      \big\vert i=1,\ldots, m \right)\)
\item The likelihood for data  is taken to be a normal distribution
$$ p(\mathcal{D}| f) = \prod_{i=1}^m 
      \mathcal{N}\left(y_i \big\vert f(\bm{x}_i), \sigma^2\right) $$
\begin{itemize}
\item We assume that \(f(\bm{x}_i)\) is the true value of the function
at \(\bm{x}_i\), but, because of measurement error, \(y_i\) will be
distributed according to a normal distribution
\item \(\sigma\) is the accuracy of our measured data (it is another
hyper-parameter we must choose)
\item Note we are assuming the data points are independent of each other
(which is usually a good approximation)
\end{itemize}
\item We can now compute a posterior
$$ p(f|\mathcal{D}, m, k, \sigma) = \frac{p(\mathcal{D}| f, \sigma)  \,
     p(f|m,k)}{p(\mathcal{D}|m,k,\sigma)}  $$ 
\begin{itemize}
\item I have explicitly written in the dependencies on they
hyper-parameters \(m\), \(k\) and \(\sigma\)
\item The Gaussian Process prior is a conjugate distribution for the
normal likelihood
\item Computing probabilities over functions is slightly wild
\end{itemize}
\item It is more useful to compute the probability at a particular
point \(\bm{x}^*\)
\begin{itemize}
\item To compute this we marginalise over all other points
\begin{itemize}
\item This is non-trivial but doable
\item If you have a multivariate normal distribution
$$ \mathcal{N}(\bm{x}| \bm{\mu},\mat{C}) =
         \frac{1}{\sqrt{|2\,\pi\, \mat{C}|}} \e{-\tfrac{1}{2}
         (\bm{x}-\bm{\mu})^\tr \mat{C}\, (\bm{x}-\bm{\mu})} $$
and you integrate over \(x_i\) (marginalise it out) then
$$ \int_{-\infty}^{\infty} \mathcal{N}(\bm{x}|
         \bm{\mu},\mat{C}) \, \dd x_i = \mathcal{N}(\hat{\bm{x}}|
         \hat{\bm{\mu}},\hat{\mat{C}}) $$
where \(\hat{\bm{x}}\) and \(\hat{\bm{\mu}}\) are identical to
\(\bm{x}\) and \(\bm{\mu}\) except with their \(i^{th}\) component
removed and \(\hat{\mat{C}}\) is identical to \(\mat{C}\) except
with the \(i^{th}\) row and column removed
\item Intuitively this seems very natural (if \(x_i\) can take any value it
doesn't change the mean or covariance between other random
variables )
\item You can prove this algebraically but it is not a trivial calculation
\item Because of this for a Gaussian Process we only care about
those set of points where we have data values
\end{itemize}
\item For all point \(\bm{x}\) (except the point \(\bm{x}^*\)) we
integrate over the possible values that \(f(\bm{x})\) can take
\item Where we have no prior this integral is just a constant---and
will cancel with the denominator \(p(\mathcal{D}|m,k,\sigma)\)
\item We are left with
\begin{align*}
p(f(\bm{x}^*)|\mathcal{D}) \propto \left(\prod_i \int \dd
f(\bm{x}_i)\right) &
\e{- \tfrac{1}{2\sigma^2} \sum\limits_{i=1}^m (y_i-f(\bm{x}_i))^2 } 
\, \e{-\tfrac{1}{2}\sum\limits_{i,j=1}^m
(f(\bm{x}_i)-m(\bm{x}_i))\, k^{-1}(\bm{x}_i,\bm{x}_j)  \,
(f(\bm{x}_j)-m(\bm{x}_j))}\\
& \e{-\sum\limits_{j=1}^m
(f(\bm{x}^*)-m(\bm{x}^*))\, k^{-1}(\bm{x}^*,\bm{x}_j)  \,
(f(\bm{x}_j)-m(\bm{x}_j)) } \\
& \e{ - \tfrac{1}{2} (f(\bm{x}^*)-m(\bm{x}^*))\, k^{-1}(\bm{x}^*,\bm{x}^*)
(f(\bm{x}^*)-m(\bm{x}^*))}
\end{align*}
\begin{itemize}
\item Writing \(f^*=f(\bm{x}^*)\), \(f_i=f(\bm{x}_i)\), \(m_i=m(\bm{x}_i)\),
\(k^{-1}_{ij} =k^{-1}(\bm{x}_i,\bm{x}_j)\),
\(k^{*-1}_j=k^{-1}(\bm{x}^*,\bm{x}_j)\) and
\(k^{*-1}=k^{-1}(\bm{x}^*,\bm{x}^*)\)  then
\begin{align*}
p(f^*|\mathcal{D}) \propto  \e{-\tfrac{1}{2} k^{*-1}(f^*-m^*)^2} 
 \left(\prod_i \int \dd f_i\right) &
   \e{- \tfrac{1}{2\sigma^2} \sum\limits_{i=1}^m (y_i-f_i)^2
    -\tfrac{1}{2}\sum\limits_{i,j=1}^m
  (f_i-m_i)\, k^{-1}_{ij}  \, (f_j-m_j)} \\
  & \e{ -\sum\limits_{j=1}^m
  (f^*-m^*)\, k^{*-1}_{j}  \, (f_j-m_j) }
  \end{align*}
\item Doing Gaussian integrals is a pain---\emph{I don't expect you to do this}
\item But if you are brave enough to try let \(g_i=f_i-m_i\), \(g^*=f^*-m^*\) and
\(\hat{y_i} = y_i-m_i\) then we can rewrite this as
$$ p(f^*|\mathcal{D}) \propto \e{-\tfrac{1}{2} k^{*-1}g^{*2}}
           \left(\prod_i \int \dd g_i\right) 
            \e{- \tfrac{1}{2\sigma^2} \sum\limits_{i=1}^m (\hat{y}_i-g_i)^2
             -\tfrac{1}{2}\sum\limits_{i,j=1}^m
           g_i\, k^{-1}_{ij}  \, g_j -  g^* \sum\limits_{j=1}^m k^{*-1}_i \,g_j} $$
\begin{itemize}
\item where we made a change of variable in the integral
\end{itemize}
\item In matrix form we can write this as
$$ p(f^*|\mathcal{D}) \propto \e{-\tfrac{1}{2} k^{*-1}g^{*2}}
         \left(\prod_i \int  \dd g_i\right) 
         \e{(\sigma^{-2}\, \hat{\bm{y}} - g^* \bm{k}^{*-1})^\tr \,\bm{g} -\tfrac{1}{2}
         \bm{g}^\tr \, (\mat{k}^{-1}-\sigma^{-2}\mat{I}) \,\bm{g} } $$
\begin{itemize}
\item I've dropped a term \(\e{-\frac{ \|
           \hat{\bm{y}}\|^2}{2\,\sigma^2}}\) since this is a constant
that can be absorbed into the constant of proportionality
\end{itemize}
\item To perform the integrals over \(g_i\) we \emph{complete the
square}.  To make the algebra easier to follow we define
\(\mat{M}=\mat{k}^{-1}-\sigma^{-2}\mat{I}\) and
\(\bm{b}=\sigma^{-2}\, \hat{\bm{y}} - g^* \bm{k}^{*-1}\).  We
now rewrite  the exponent, \(E\), of the  integral as
\begin{align*}
E &= (\sigma^{-2}\, \hat{\bm{y}} - 
g^* \bm{k}^{*-1})^\tr \,\bm{g} -\frac{1}{2}
\bm{g}^\tr  (\mat{k}^{-1}-\sigma^{-2}\mat{I})\, \bm{g}
=  \bm{b}^\tr \,\bm{g} -\frac{1}{2}
\bm{g}^\tr \mat{M} \bm{g} \\
&=   -\frac{1}{2} (\bm{g} - \mat{M}^{-1}\bm{b})^\tr \mat{M}\,
(\bm{g} - \mat{M}^{-1} \bm{b})
+ \frac{1}{2} \bm{b}^\tr \mat{M}^{-1}\bm{b}
\end{align*}
\begin{itemize}
\item You simply have to expand out all the terms to see these
are the same
\end{itemize}
\item Defining \(\bm{h} = \bm{g} - \mat{M}^{-1}\bm{b}\) and
substituting this back into  \(p(f^*|\mathcal{D})\) we get
$$ p(f^*|\mathcal{D}) \propto \e{-\tfrac{1}{2} k^{*-1}g^{*2}+
         \frac{1}{2} \bm{b}^\tr \mat{M}^{-1}\bm{b} }
         \left(\prod_i \int  \dd h_i\right) \e{-\tfrac{1}{2}
         \bm{h}^\tr\mat{M}\, \bm{h}} $$
\item The last integral is a Gaussian integral that integrates to a
constant (and can be absorbed into the constant of proportionality)
\item We are left with
 \begin{align*}
 p(f^*|\mathcal{D}) &\propto \e{-\tfrac{1}{2} k^{*-1}g^{*2}+
 \frac{1}{2} \bm{b}^\tr \mat{M}^{-1}\bm{b}} \\
 &= \e{-\tfrac{1}{2} k^{*-1}g^{*2}+ \tfrac{1}{2}
 (\sigma^{-2}\, \hat{\bm{y}} - g^* \bm{k}^{*-1})^\tr
 \mat{M}^{-1}
 (\sigma^{-2}\, \hat{\bm{y}} - g^* \bm{k}^{*-1})}\\
& \propto \e{-\frac{1}{2}(k^{*-1} - \bm{k}^{*-1\tr}
\mat{M}^{-1} \bm{k}^{*-1}) g^{*2}
 - \frac{g^*}{\sigma^2} \hat{\bm{y}}^\tr \mat{M}^{-1} \bm{k}^{*-1}}
 \end{align*}
\begin{itemize}
\item where we have used \(\bm{b}=\sigma^{-2}\, \hat{\bm{y}} - g^*
           \bm{k}^{*-1}\)
\item we have also thrown away terms not involving \(g^*\) (as they
are constants that can be absorbed into the constant of proportionality)
\end{itemize}
\item We can again \emph{complete the square} this time for \(g^*\)
$$ p(f^*|\mathcal{D}) \propto \e{-\frac{1}{2}(k^{*-1} - \bm{k}^{*-1\tr}     \mat{M}^{-1} \bm{k}^{*-1}) (g^{*}- \frac{\hat{\bm{y}}^\tr \mat{M}^{-1} \bm{k}^{*-1}}{\sigma^2(k^{*-1} - \bm{k}^{*-1\tr}  \mat{M}^{-1} \bm{k}^{*-1})})} $$
\begin{itemize}
\item We've again dropped terms that don't contain \(g^*\)
\end{itemize}
\item But \(g^*=f^*-m^*\) so we can write this as
\begin{align*}
p(f^*|\mathcal{D}) &\propto \e{-\frac{1}{2}(k^{*-1} -
\bm{k}^{*-1\tr}     \mat{M}^{-1} \bm{k}^{*-1}) (f^{*}-
m^* -   \frac{\hat{\bm{y}}^\tr \mat{M}^{-1}
\bm{k}^{*-1}}{\sigma^2(k^{*-1} - \bm{k}^{*-1\tr}
\mat{M}^{-1} \bm{k}^{*-1})})} \\
& \propto \mathcal{N}\!\left(f^*\bigg\vert m^* +  \frac{\hat{\bm{y}}^\tr \mat{M}^{-1}
\bm{k}^{*-1}}{\sigma^2(k^{*-1} - \bm{k}^{*-1\tr}
\mat{M}^{-1} \bm{k}^{*-1})}, k^{*-1} -
\bm{k}^{*-1\tr}     \mat{M}^{-1} \bm{k}^{*-1} \right)
\end{align*}
\begin{itemize}
\item where we use the fact that we end up with a term that has
the form of a normal distribution
\item because the posterior term is normalise, in fact it has to
be exactly equal to this normal distribution
\end{itemize}
\item Using \(\mat{M}=\mat{k}^{-1}-\sigma^{-2}\mat{I}\) 
$$ \hspace{-2cm} p(f^*|\mathcal{D}) =
	 \mathcal{N}\!\left(f^*\bigg\vert m^* +
         \frac{\hat{\bm{y}}^\tr
         \left(\mat{k}^{-1}-\sigma^{-2}\mat{I}\right)^{-1} 
         \bm{k}^{*-1}}{\sigma^2(k^{*-1} - \bm{k}^{*-1\tr}
         \left(\mat{k}^{-1}-\sigma^{-2}\mat{I}\right)^{-1}
         \bm{k}^{*-1})},\, \frac{1}{
        k^{*-1} - \bm{k}^{*-1\tr}
         \left(\mat{k}^{-1}-\sigma^{-2}\mat{I}\right)^{-1}
         \bm{k}^{*-1} } \right) $$
\item In other words the expected mean value of \(f^*=f(\bm{x}^*)\)
is
$$ \av{f(\bm{x}^*)} = m(\bm{x}^*) +
         \frac{\hat{\bm{y}}^\tr
         \left(\mat{k}^{-1}-\sigma^{-2}\mat{I}\right)^{-1} 
         \bm{k}^{*-1}}{\sigma^2(k^{*-1} - \bm{k}^{*-1\tr}
         \left(\mat{k}^{-1}-\sigma^{-2}\mat{I}\right)^{-1}
         \bm{k}^{*-1})} $$
(note that usually \(m(\bm{x}^*)=0\))
and the expected variance in the value is
$$ \mathbb{V}\mathrm{ar}[f(\bm{x}^*)] 
         = \frac{1}{k^{*-1} - \bm{k}^{*-1\tr}
         \left(\mat{k}^{-1}-\sigma^{-2}\mat{I}\right)^{-1}
         \bm{k}^{*-1}} $$
\item The result is pretty horrible because it involves inverting
a matrix \(\mat{K}\) with components \(k(\bm{x},\bm{y})\)
evaluated at the set of points
\(\{\bm{x}_1,\bm{x}_2,\ldots,\bm{x}_m,\bm{x}^*\}\)
\item This is usually feasible but there is a simpler form
that occurs because of identities involving inverse matrices
\item This is best obtained using a second way of deriving the results
\item I went to the pain of deriving the result this way because it
is just a consequence of Bayes' rule
\end{itemize}
\end{itemize}
\item In the second derivation we consider joint probability of the
observations \(\{y_i| i=1,2,\ldots,m\}\) and \(f^*=f(\bm{x}^*)\)
\begin{itemize}
\item Even this derivation is a pain and you are \textbf{not} expected to
know it
\item Now we assume \(y_i = f(\bm{x}_i) + \epsilon_i\) where
\(\epsilon_i\sim \mathcal{N}(0,\sigma^2)\) (that is our
observations have a normally distributed error) then
$$ \av{y_i \, y_j} =  \av{(f(\bm{x}_i) +
       \epsilon_i)(f(\bm{x}_j) + \epsilon_j)} =
       k(\bm{x}_i,\bm{x}_j) + \sigma^2 \,\delta_{ij} $$
\begin{itemize}
\item since the observation error, \(\epsilon_i\), is assumed independent of the
function value, \(f(\bm{x}_i)\)
\item here we assume \(m(\bm{x})=0\) just to make life simple
\end{itemize}
\item Similarly
$$ \av{y_i\,f^*} =  \av{(f(\bm{x}_i) + \epsilon_i) f(\bm{x}^*)} 
       = k(\bm{x}_i,\bm{x}^*) $$
and
$$ \av{(f^*)^2} = k(\bm{x}^*,\bm{x}^*) $$
\item Thus
$$ p(f^*, \bm{y}) = \mathcal{N}\!\left( \begin{pmatrix}  \bm{y}  \\ f_*  \end{pmatrix}
        \bigg \vert \bm{0} ,
        \begin{pmatrix}
        \mat{K} + \sigma^2 \, \mat{I} & \bm{k}^* \\
         \bm{k}^{*\tr} & k^*
        \end{pmatrix}\right) $$
\begin{itemize}
\item where \(\mat{K}\) is a matrix with components
\(k(\bm{x}_i,\bm{x}_j)\), \(\bm{k}^*\) is a vector with
components \(k(\bm{x}_i,\bm{x}^*)\) and \(k^*=k(\bm{x}^*,\bm{x}^*)\)
\end{itemize}
\item We now use an identity involving matrix inverses
$$  \begin{pmatrix}
        \mat{K} + \sigma^2 \, \mat{I} & \bm{k}^* \\
         \bm{k}^{*\tr} & k^*
        \end{pmatrix}^{-1}
	= \begin{pmatrix}
        \left(\mat{K} + \sigma^2 \, \mat{I} -
       \frac{\bm{k}^{*\tr}\bm{k}^*}{k^*}\right)^{-1}
       & -\frac{ (\mat{K} + \sigma^2 \,
       \mat{I})^{-1}\bm{k}^*}{k^*-\bm{k}^{*\tr}(\mat{K} + \sigma^2 \,
       \mat{I})^{-1}\bm{k}^*} \\
       -\frac{ \bm{k}^{*\tr}(\mat{K} + \sigma^2 \, \mat{I})^{-1}}{k^*-\bm{k}^{*\tr}(\mat{K} + \sigma^2 \, \mat{I})^{-1}\bm{k}^*} 
         & \frac{1}{k^*-\bm{k}^{*\tr}(\mat{K} + \sigma^2 \, \mat{I})^{-1}\bm{k}^*} 
        \end{pmatrix} $$
\begin{itemize}
\item these identities involving inverses of matrices seem to come
from nowhere---they make working with normal distributions a
real pain
\item you can prove the identity by multiply the right-hand side by \(\begin{pmatrix}
        \mat{K} + \sigma^2 \, \mat{I} & \bm{k}^* \\
         \bm{k}^{*\tr} & k^*
        \end{pmatrix}\) and showing you get the identity
\item if you are tempted to do this then set \(\sigma^2=0\) and
rename \(\bm{k}^*\) and \(k^*\) so its easier to do the
algebra---it is a tedious calculation (see exercises)
\end{itemize}
\item Now we use \(p(f^*|\mathcal{D}) = p(f^*|\bm{y}) =
       p(f^*,\bm{y})/p(\bm{y})\)
\item All you need to do is collect the terms in \(p(f^*,\bm{y})\)
involving \(f^*\)
$$ p(f^*|\mathcal{D})  \propto
       \e{-\frac{f^{*2}}{2(k^*-\bm{k}^{*\tr}(\mat{K} + \sigma^2 \, \mat{I})^{-1}\bm{k}^*)}  + \frac{ f^*\,\bm{k}^{*\tr}(\mat{K} + \sigma^2 \, \mat{I})^{-1}\bm{y}}{k^*+\bm{k}^{*\tr}(\mat{K} + \sigma^2 \,  \mat{I})^{-1}\bm{k}^*}  } $$
\item \emph{Completing the square}
$$ p(f^*|\mathcal{D})  \propto
       \e{-\frac{1}{2(k^*-\bm{k}^{*\tr}(\mat{K} + \sigma^2 \,
       \mat{I})^{-1}\bm{k}^*)} \left( f^* - \bm{k}^{*\tr}(\mat{K} +
       \sigma^2 \, \mat{I})^{-1}\bm{y}\right)^2} $$
\item But as \(p(f^*|\mathcal{D})\) must be normalised
$$ p(f(\bm{x}^*)|\mathcal{D})  = \mathcal{N}\left(f(\bm{x}^*)
       \bigg\vert \bm{k}^{*\tr}(\mat{K} +
       \sigma^2 \, \mat{I})^{-1}\bm{y},\, 
       k^*-\bm{k}^{*\tr}(\mat{K} + \sigma^2 \,
       \mat{I})^{-1}\bm{k}^*\right) $$
\item That is the mean value of \(f\) at a point \(\bm{x}^*\) is
$$ \av{f(\bm{x}^*)} =  m(\bm{x}^*) + \bm{k}^{*\tr}(\mat{K} +
       \sigma^2 \, \mat{I})^{-1}\bm{y} $$
\begin{itemize}
\item I've re-instated \(m(\bm{x}^*)\) (even though it is usually
taken to be zero)
\end{itemize}
\item The variance is given by
$$ \mathbb{V}\mathrm{ar}[f(\bm{x}^*)]  = k^*-\bm{k}^{*\tr}(\mat{K} + \sigma^2 \,
       \mat{I})^{-1}\bm{k}^* $$
\item This may look completely different from the previous expression
but surprisingly it is identical (you can demonstrate this numerically)
\item The advantage over the previous methods is that we only need to
do one matrix inversion (and we get a slightly easier expression)
\item Although the mathematics to get here was horrible the
expression is extremely easy to compute
\item One of the advantages of Bayesian inference is it provides an
estimate of its own uncertainty (in this case \(\mathbb{V}\mathrm{ar}[f(\bm{x}^*)]\))
\end{itemize}
\end{itemize}

\subsection{Learning Hyperparameters}
\label{sec:orgbc635cc}
\begin{itemize}
\item \textbf{Hyperparameters in the Bayesian Framework}
\begin{itemize}
\item As I have said many times to get machine learning to work it is
vital to choose the hyperparameter properly (we will see this
is true for GPs)
\item In the Bayesian framework if \(\bm{\phi}\) are hyperparameters
then Bayes' rules says
$$ p(\bm{x}|\mathcal{D},\bm{\phi}) =
       \frac{p(\mathcal{D}|\bm{x},\bm{\phi}) \,
       p(\bm{x}|\bm{\phi})}{p(\mathcal{D}|\bm{\phi}) } $$
\begin{itemize}
\item this is just Bayes' rule with everything conditioned on the
hyperparameters \(\bm{\phi}\)
\end{itemize}
\item We can select hyperparameters by considering the \textbf{evidence},
\(p(\mathcal{D}|\bm{\phi})\)
\begin{itemize}
\item when the hyperparameters are viewed as different models this is
known as \textbf{model selection}
\item this is also called the evidence framework
\item We can view the relatively likelihood of one model,
\(\bm{\phi}_1\) compare to a second, \(\bm{\phi}_2\) by examining
$$ \frac{p(\mathcal{D}|\bm{\phi}_1)}{p(\mathcal{D}|\bm{\phi}_2)} $$
\end{itemize}
\item If we want to be hyper-Bayesian then we can put a prior,
\(p(\bm{\phi})\), over our hyperparameters and the calculate the
joint posterior for the parameters of the likelihood
\(\bm{\theta}\) and the hyper-parameters \(\bm{\phi}\)
$$ p(\bm{\theta},\bm{\phi}| \mathcal{D}) = \frac{p(\mathcal{D}|
       \bm{\theta},\bm{\phi}) \, p(\bm{\theta}|\bm{\phi}) \, p(\bm{\phi})}{
       p(\mathcal{D})} $$
\begin{itemize}
\item to compute the posterior we are interested in we marginalise
out the hyperparameters
$$ p(\bm{\theta}|\mathcal{D}) = \int
         p(\bm{\theta},\bm{\phi}| \mathcal{D}) \,\dd \bm{\phi} $$
\item Often this integral is not computable in closed form and we
are forced to estimate it using Monte-Carlo techniques
\end{itemize}
\end{itemize}
\item \textbf{Hyperparameter for Gaussian Processes}
\begin{itemize}
\item For Gaussian Processes this means choosing the right kernel function
\item Here we are in a better positions than in SVMs in that the kernel
represents the covariance function
\item Given data we could estimate the kernel function using
$$ \av{y_i,y_j} = k(\bm{x}_i,\bm{x}_j) + \sigma^2 \delta_{ij} $$
\item It would be an inverse problem to estimate \(k(\bm{x}_i,\bm{x}_j)\)
\item Often we start with a guess of the form of the kernel
\item A very common kernel function is the Gaussian kernel
$$ k(\bm{x},\bm{y}) = \e{-\frac{\|\bm{x}-\bm{y}\|^2}{2\ell^2}} $$
\item Now \(\ell\) is a \emph{scale parameter} that has to be determined
\item However, a great advantage of Gaussian Processes is that we can
compute the evidence in closed form
$$  \logg{p(\mathcal{D}|\bm{\theta})} = - \frac{1}{2} \bm{y}^\tr (\mat{K}+\sigma^2\mat{I})^{-1} \bm{y} - \frac{1}{2}  \logg{|\mat{K}+\sigma^2\mat{I}|} - \frac{m}{2} \log(2\,\pi) $$
\begin{itemize}
\item This allows us to rapidly compare different hyperparameters
(kernels and noise level)
\item We are not doing this in a fully Bayesian way (we are
maximising our evidence rather than updating a distribution
for the hyperparameters) therefore we could overfit
\item However, usually we have relatively few hyperparameters so
overfitting is less severe
\end{itemize}
\end{itemize}
\end{itemize}

\section{Exercises}
\label{sec:org07031ce}

\subsection{Working with Gaussians}
\label{sec:org393fd1c}

This is optional.  Learning to work with Gaussians allows you to
understand a lot of machine learning techniques, but it is not
going to be examined.

\begin{enumerate}
\item Integrals involving Gaussians can be done in closed form.  This
means that they are heavily used in machine learning.  It does,
however, take a lot of practice to learn how to do these
integrals.  The starting point is the integral
\begin{align*}
 I_1 = \int_{-\infty}^{\infty} \e{-x^2/2} \, \dd x = \sqrt{2\,\pi}.
\end{align*}
This integral isn't easy.  The trick is to turn this into a two
dimensional integral and perform the integral in polar
coordinates. Have a go.
\item Show by a change of variables
\begin{align*}
I_2 = \int_{-\infty}^{\infty} \e{-x^2/(2\,\sigma^{2})} \, \dd x = \sqrt{2\,\pi}\,\sigma.
\end{align*}
\item Show that
\begin{align*}
I_3 = \int_{-\infty}^{\infty} \e{-x^2/2 + a \,x} \, \frac{\dd x}{\sqrt{2\,\pi}}
= \e{a^2/2}
\end{align*}
This involves completing the square and using a change of
variables.  It is probably the most frequently used trick
working with Gaussian integrals.
\item Show that
\begin{align*}
I_4 &= \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty}
 \e{-\tfrac{1}{2} \|\bm{x}\|^2} 
\dd \bm{x} = (2\,\pi)^{\tfrac{n}{2}}
\end{align*}
Often we don't bother writing all the integral signs (one will do)
\item By writing \(\mat{M} =
      \mat{V}\,\mat{\Lambda}\,\mat{V}^{\tr}\) show that
\begin{align*}
I_5 = \int_{-\infty}^{\infty}  \e{-\tfrac{1}{2} \bm{x}^{\tr} \mat{M} \bm{x}}  \dd \bm{x} = 
(2\,\pi)^{\tfrac{n}{2}} \, \frac{1}{\sqrt{|\mat{M}|}}
\end{align*}
where \(|\mat{M}|\) is the determinant of \(\mat{M}\).  Note that
\(|\mat{M}|=|\mat{\Lambda}| = \prod_i \lambda_{i}\).
\item Show that
\begin{align*}
I_6 = \int_{-\infty}^{\infty}  \e{-\tfrac{1}{2} \bm{x}^{\tr} \mat{M}^{-1} \bm{x}}  \dd \bm{x} = 
(2\,\pi)^{\tfrac{n}{2}} \, \sqrt{|\mat{M}|}
\end{align*}
\end{enumerate}


\subsection{Working with Matrix Inverses}
\label{sec:org6b35c2c}
\begin{itemize}
\item Modelling using normal (aka Gaussian) distributions is very
powerful because we can usually computer everything in closed
form---integrals involving Gaussians are always doable
\item However, there is a twist which makes it extremely painful
\begin{itemize}
\item The normal distribution uses the inverse of the covariance matrix
\item The inverse of matrices are a pain to work with
\end{itemize}
\item To get nice results we often consider partition matrices into
four blocks---the inverse of the full matrix can be written in
terms of the inverse of each block
\item Let's look at a very simple example
\begin{itemize}
\item Show that
$$ \begin{pmatrix} \mat{K} & \vl \\ \vl^\tr & m \end{pmatrix}^{-1} 
       =  \begin{pmatrix} \left(\mat{K}-\frac{\vl\,\vl^\tr}{m}\right)^{-1}  &
            -\frac{\mat{K}^{-1} \vl}{m-\vl^\tr \mat{K}^{-1}\vl}  \\ 
	    -\frac{\vl^\tr\mat{K}^{-1}}{m-\vl^\tr \mat{K}^{-1}\vl} &
	    \frac{1}{m-\vl^\tr \mat{K}^{-1}\vl} 
       \end{pmatrix} $$
\begin{itemize}
\item \(\mat{K}\) is a symmetric matrix, \(\ell\) is a vector and \(m\) a
scalar
\item Similar identities exist when \(\ell\) and \(m\) are matrices,
but let's keep thing simple
\end{itemize}
\item Now we can show this by multiply both sides by \(\begin{pmatrix}
       \mat{K} & \vl \\ \vl^\tr & m \end{pmatrix}\) (the algebra is
easier if we multiply on the right)
\item This is still very fiddly to show, so break it down
\begin{itemize}
\item Show that
$$ \begin{pmatrix} \mat{A} & \vb \\ \vb^\tr & c \end{pmatrix}
	 \begin{pmatrix} \mat{K} & \vl \\ \vl^\tr & m \end{pmatrix} =
	 \begin{pmatrix} \mat{A}\,\mat{K} + \vb\,\vl^\tr &
         \mat{A}\,\vl + m\,\vb \\ 
         \vb^\tr\mat{K} + c\,\vl^\tr & \vb^\tr \vl + c\,m \end{pmatrix} $$
\item Now you have to show that when \(\mat{A} = 
         \left(\mat{K}-\frac{\vl\,\vl^\tr}{m}\right)^{-1}\),
\(\vb=-\frac{\mat{K}^{-1} \vl}{m-\vl^\tr \mat{K}^{-1}\vl}\)
and \(c=\frac{1}{m-\vl^\tr \mat{K}^{-1}\vl}\) that
\begin{enumerate}
\item \(\mat{A}\,\mat{K} + \vb\,\vl^\tr = \mat{I}\)
\item \(\mat{A}\,\vl + m\,\vb = \bm{0}\)
\item \(\vb^\tr\mat{K} + c\,\vl^\tr = \bm{0}^\tr\)
\item \(\vb^\tr \vl + c\,m = 1\)
\end{enumerate}
\item To show 1. and 2. take out \(\mat{A}\) as a common factor using
\(\mat{I}=\mat{A}\,\mat{A}^{-1}\) (answer below)
\end{itemize}
\end{itemize}
\end{itemize}

\section{Experiments}
\label{sec:orgd904c60}

\subsection{Generate Samples from a Gaussian Processes}
\label{sec:orgb9df890}
\begin{itemize}
\item To generate a Gaussian Process, \(\mathcal{GP}(0,k)\), with
\(m(\bm{x})=0\) and covariance
$$ k(\bm{x},\bm{y})=\e{-\frac{\|\bm{x}-\bm{y}\|^2}{2\,\ell^2}} $$
a points \(\{\bm{x}_i|i=1, 2, \ldots, n\}\)
\begin{enumerate}
\item We use first compute the covariance matrix \(\mat{K}\) with
elements \(K_{ij} = k(\bm{x}_i,\bm{x}_j)\)
\item Compute the \emph{Cholesky decompostion} \(\mat{L}\) such that
\(\mat{K} = \mat{L}\,\mat{L}^\tr\)
\begin{itemize}
\item For any positive definite matrices we can always compute a
Cholesky decomposition (although beware, due to numerical
rounding some matrices that should be positive definite,
appear not to have Cholesky decompositions---you can
\(\epsilon \,\mat{I}\) to your matrix to help)
\item The Cholesky decomposition is a lower diagonal matrix
\item It is used to efficiently solve linear problems involving
positive definite matrices (it is more efficient and more
stable than Gaussian elimination)
\end{itemize}
\item Now let \(\bm{f} = \mat{L}\,\bm{\eta}\) where \(\bm{\eta}\sim
        \mathcal{N}(\bm{0},\mat{I})\)
\begin{itemize}
\item We note that
$$ \av{\bm{f}\,\bm{f}^\tr} = \av{\mat{L}\,\bm{\eta}\,\bm{\eta}^\tr\mat{L}^\tr}
	  = \mat{L}\,\av{\bm{\eta}\,\bm{\eta}^\tr} \mat{L}^\tr
	  = \mat{L}\,\mat{I}\,\mat{L}^\tr =\mat{L}\,\mat{L}^\tr =
          \mat{K} $$
\item Thus \(\bm{f}\) can be viewed as samples from a Gaussian Process
\item Draw many different samples and plot them
\end{itemize}
\item Experiment with different values of \(\ell\)
\end{enumerate}
\end{itemize}

\textbf{1-d Gaussian Processes}

\begin{minted}[]{octave}
x = [-10:0.2:10];    % define some points
n = length(x)
l = 1.0                      % define length scale
K = zeros(n,n);         % define holder
for i = 1:n
  K(i,i) = 1;
  for j = 1:i
    K(j,i) = K(i,j) = exp(-(x(i)-x(j))^2/(2*l)); % define covariance matrix
  endfor
endfor
L = chol(K+0.0001*eye(n));  % cheat to persuade octave K is positive definite
f1 = L*randn(n,1);
f2 = L*randn(n,1);
f3 = L*randn(n,1);
plot(x,f1,x,f2,x,f3)
\end{minted}

\textbf{2-d Gaussian Process}

\begin{minted}[]{octave}
range = [-4:0.2:4];    % define some points
n = length(range)
X = zeros(n*n,2);
for i = 1:n
  for j = 1:n
     X((i-1)*n+j,1) = range(i);
     X((i-1)*n+j,2) = range(j);
  endfor
endfor
l = 1.0                      % define length scale
N =length(X)
K = zeros(N,N);  
for i = 1:N
  K(i,i) = 1;
  for j = 1:i
    K(j,i) = K(i,j) = exp(-norm(X(i,:)-X(j,:))^2/(2*l)); % define covariance matrix
  endfor
endfor

L = chol(K+0.0001*eye(N));  % cheat to persuade octave K is positive definite
f = L*randn(N,1);
fm = reshape(f,n,n);
[xx, yy] = meshgrid(range, range);
mesh(xx, yy, fm);
xlabel ("x");
ylabel ("y");
zlabel ("f(x,y)");
title ("2-d Gaussian Process");
\end{minted}



\section{Answers}
\label{sec:orgb818bb4}
\subsection{Working with Gaussians}
\label{sec:org29c0b44}
\begin{enumerate}
\item The Gaussian Integral.
\begin{itemize}
\item We consider the two dimensional integral
 \begin{align*}
   I_1^2 &= \left( \int_{-\infty}^{\infty} \e{-x^2/2} \, \dd x \right)^2 
   \eq \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
    \e{-x^2/2-y^2/2} \, \dd x  \, \dd y
   \\
    &\eq \int_0^{\infty} \int_0^{2\pi} r\, \e{-r^2} \dd r\, \dd
    \theta
   \eq  \int_0^{\infty} r\, \e{-r^2} \dd r \int_0^{2\pi} \, \dd
    \theta = 2\,\pi
\end{align*}
\begin{enumerate}
\item Changing the name of the dummy index from \(x\) to \(y\)
\item Using the change of variable \(x = r\,\cos(\theta)\) and
\(y=r\,\sin(\theta)\) so that \(\dd x \dd y = r \dd r\, \dd
           \theta\)
\item Reordering the integrals and using \(\int_0^{\infty} r\,
           \e{-r^2} \dd r=1\) (which you can prove by a change of
variables   \(u = r^2/2\) so that \(r\,\dd r = \dd u\))
\end{enumerate}
Therefore \(I_1 = \sqrt{2\,\pi}\).
\end{itemize}
\item \reseteq
\begin{align*}
I_2 = \int_{-\infty}^{\infty} \e{-x^2/(2\,\sigma^{2})} \, \dd x 
\eq \sigma \, \int_{-\infty}^{\infty} \e{-u^2/2} \, \dd u \eq \sqrt{2\,\pi}\,\sigma.
\end{align*}
\begin{enumerate}
\item Using \(u = x/\sigma\) so that \(\dd x = \sigma\,\dd u\)
\item Using \(I_{1}\)
\end{enumerate}
\item \reseteq
\begin{align*}
I_2 &=  \int_{-\infty}^{\infty} \e{-x^2/2 + a \,x} \, \frac{\dd x}{\sqrt{2\,\pi}}
\eq  \int_{-\infty}^{\infty} \e{-\tfrac{1}{2}(x- a)^2 + \tfrac{a^2}{2}} \, \frac{\dd x}{\sqrt{2\,\pi}}
\\
&\eq  \e{a^2/2} \int_{-\infty}^{\infty} \e{-\tfrac{1}{2}(x- a)^2} \, \frac{\dd x}{\sqrt{2\,\pi}}
\eq \e{a^2/2}
\end{align*}
\begin{enumerate}
\item Using \(-x^2/2 + a \,x = -\tfrac{1}{2}(x- a)^2 +
         \tfrac{a^2}{2}}\)
\item Factoring out \(\e{a^2/2}\)
\item Making a change of variables \(u=x-a\) and using integral \(I_1\)
\end{enumerate}
\item \reseteq
\begin{align*}
I_4 &= \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty}
 \e{-\tfrac{1}{2} \|\bm{x}\|^2} 
\dd \bm{x}
\eq \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty}
\e{-\tfrac{1}{2} \sum\limits_{i=1}^{n} x_i^2}
\, \prod_{i=1}^n \dd x_{i}
\\
&\eq  \prod_{i=1}^n \left( \int_{-\infty}^{\infty} \e{-\tfrac{x_i^2}{2}}
\, \dd x_{i} \right) \eq  (2\,\pi)^{\tfrac{n}{2}}
\end{align*}
\begin{enumerate}
\item Using \(\|\bm{x}\|^2=\sum\limits_{i=1}^{n} x_i^2\)
\item Using \(\exp(\sum_i a_i) = \prod_i \e{a_i}\) and factoring out
the different terms
\item Using integral \(I_1\)
\end{enumerate}
\item \reseteq
\begin{align*}
I_5 &= \int_{-\infty}^{\infty}  \e{-\tfrac{1}{2} \bm{x}^{\tr}
\mat{M} \bm{x}}  \dd \bm{x}
\eq \int_{-\infty}^{\infty}  \e{-\tfrac{1}{2} \bm{x}^{\tr}
\mat{V}\,\mat{\Lambda}\,\mat{V}^{\tr} \bm{x}}  \dd \bm{x}
\\
&\eq \int_{-\infty}^{\infty}  \e{-\tfrac{1}{2} \bm{u}^{\tr}
\,\mat{\Lambda}\, \bm{u}} \,  \dd \bm{u}
\eq \int_{-\infty}^{\infty}  \e{-\tfrac{1}{2} \sum_{i} \lambda_i u_i^2}\,  \dd \bm{u}
\\
&\eq \prod_{i} \left(  \int_{-\infty}^{\infty}  \e{-\tfrac{1}{2} \lambda_i u_i^2}\,  \dd u_i \right)
\eq \prod_i \left( \sqrt{\frac{2\pi}{\lambda_i}} \right)
\eq (2\,\pi)^{n/2} \frac{1}{\sqrt{\prod_i \lambda_i}}
\eq   \, \frac{(2\,\pi)^{\tfrac{n}{2}}}{\sqrt{|\mat{M}|}}
\end{align*}
\begin{enumerate}
\item Using \(\mat{M} =  \mat{V}\,\mat{\Lambda}\,\mat{V}^{\tr}\) where
\item Writing \(\bm{u} = \mat{V}^{\tr} \bm{x}\).  Note that \(\dd
         \bm{u} = |\mat{V}^{\tr}|\, \dd \bm{x}\), but \(\mat{V}\) is an
orthogonal matrix so \(|\mat{V}^{\tr}|=1\). So \(\dd \bm{u} = \dd \bm{x}\).
\item \(\mat{\Lambda}\) is a diagonal matrix with elements \(\Lambda_{ii}=\lambda_i\)
\item Rearranging
\item Using integral \(I_2\) with \(\sigma = 1/\sqrt{\lambda_i}\)
\item Rearranging
\item Using \(\prod_i \lambda_{i} = |\mat{M}|\).
\end{enumerate}
\item This follows using \(I_5\) because \(|\mat{M}^{-1}|=\frac{1}{|\mat{M}|}\).
\end{enumerate}


\subsection{Working with Matrix Inverses}
\label{sec:org70578d1}
\begin{itemize}
\item Using \(\mat{A} = 
         \left(\mat{K}-\frac{\vl\,\vl^\tr}{m}\right)^{-1}\),
\(\vb=-\frac{\mat{K}^{-1} \vl}{m-\vl^\tr \mat{K}^{-1}\vl}\)
and \(c=\frac{1}{m-\vl^\tr \mat{K}^{-1}\vl}\), we show the four identities
\begin{enumerate}
\item \begin{align*}
\mat{A}\,\mat{K} + \vb\,\vl^\tr
&= \mat{A}\,\left(\mat{K} + \mat{A}^{-1} \vb\,\vl^\tr\right) \\
&=\left(\mat{K}-\frac{\vl\,\vl^\tr}{m}\right)^{-1} \,\left(\mat{K} +
\left(\mat{K}-\frac{\vl\,\vl^\tr}{m}\right)
 \frac{(-\mat{K}^{-1} \vl\,\vl^\tr)}{m-\vl^\tr \mat{K}^{-1}\vl} \right) \\
 &= \left(\mat{K}-\frac{\vl\,\vl^\tr}{m}\right)^{-1} \,\left(\mat{K} 
- \frac{\vl\,\vl^\tr}{m-\vl^\tr \mat{K}^{-1}\vl}
+ \frac{\vl\,\vl^\tr\mat{K}^{-1}\vl\,\vl^\tr/m}{m-\vl^\tr \mat{K}^{-1}\vl} \right)\\
&= \left(\mat{K}-\frac{\vl\,\vl^\tr}{m}\right)^{-1} \,\left(\mat{K} 
- \frac{\vl\,\vl^\tr}{m} 
  \frac{m-\vl^\tr \mat{K}^{-1}\vl}{m-\vl^\tr \mat{K}^{-1}\vl}\right) \\
&= \left(\mat{K}-\frac{\vl\,\vl^\tr}{m}\right)^{-1} \,\left(\mat{K} 
- \frac{\vl\,\vl^\tr}{m} \right) = \mat{I}
\end{align*}
\item \begin{align*}
\mat{A}\,\vl + m\,\vb 
&=  \mat{A} \left( \vl + m \,\mat{A}^{-1}\vb\right) \\
&= \mat{A} \left( \vl - m
\left(\mat{K}-\frac{\vl\,\vl^\tr}{m}\right)
 \frac{\mat{K}^{-1} \vl}{m-\vl^\tr \mat{K}^{-1}\vl} \right) \\
&=\mat{A} \left( \vl 
- \frac{m\,\vl}{m-\vl^\tr \mat{K}^{-1}\vl} (1 -\frac{\vl^\tr \mat{K}^{-1}\vl}{m}) \right)\\
&= \mat{A} \left( \vl -\vl \right) = \bm{0}
\end{align*}
\item $$ \vb^\tr\mat{K} + c\,\vl^\tr = -\frac{\vl^\tr}{m-\vl^\tr
          \mat{K}^{-1}\vl} + \frac{\vl^\tr}{m-\vl^\tr
          \mat{K}^{-1}\vl}= \bm{0}^\tr $$
\item $$ \vb^\tr \vl + c\,m =  -\frac{\vl^\tr \mat{K}^{-1} \vl}{m-\vl^\tr \mat{K}^{-1}\vl} + \frac{m}{m-\vl^\tr \mat{K}^{-1}\vl} = 1 $$
\end{enumerate}
\end{itemize}
\end{document}
