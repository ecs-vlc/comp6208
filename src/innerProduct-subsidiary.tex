% Created 2024-02-26 Mon 17:13
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[a4paper,margin=20mm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{bm}
\usepackage{minted}
\usemintedstyle{emacs}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beraserif}
\usepackage[scaled]{berasans}
\usepackage[scaled]{beramono}
\newcommand{\tr}{\textsf{T}}
\newcommand{\grad}{\bm{\nabla}}
\newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
\newcommand{\Prob}[2][]{\mathbb{P}_{#1\!}\left[ #2 \right]}
\newcommand{\logg}[1]{\log\!\left( #1 \right)}
\newcommand{\pred}[1]{\left\llbracket { \small #1} \right\rrbracket}
\newcommand{\e}[1]{{\rm e}^{#1}}
\newcommand{\dd}{\mathrm{d}}
\DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
\newcommand{\normal}[2]{\mathcal{N}\!\left(#1 \big| #2 \right)}
\newcounter{eqCounter}
\setcounter{eqCounter}{0}
\newcommand{\explanation}{\setcounter{eqCounter}{0}\renewcommand{\labelenumi}{(\arabic{enumi})}}
\newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\arabic{eqCounter})}}{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\Dist}[2][Binom]{\mathrm{#1}\left( \strut {#2} \right)}
\newcommand{\len}[1]{\left\|{#1}\right\|}
\newcommand{\inner}[2]{\left\langle{#1}, {#2}\right\rangle}
\author{Adam Prügel-Bennett}
\date{\today}
\title{Advanced Machine Learning Subsidary Notes\\\medskip
\large Lecture 8: Inner-Product Spaces}
\hypersetup{
 pdfauthor={Adam Prügel-Bennett},
 pdftitle={Advanced Machine Learning Subsidary Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.3)}, 
 pdflang={English}}
\begin{document}

\maketitle



\section{Keywords}
\label{sec:org2616c29}
\begin{itemize}
\item Inner-product, operators
\end{itemize}

\section{Main Points}
\label{sec:org3fd61a3}
\begin{itemize}
\item For some vector spaces we can define an \emph{inner product} between
pairs of vectors
\item Inner products are scalars associated with two elements in a vector space
\item They are generally denoted by \(\langle\bm{x},\bm{y}\rangle\)
\item To be an inner-product requires satisfying 5 axioms
\begin{enumerate}
\item \(\inner{\bm{x}}{\bm{x}} \geq 0\) for all \(\bm{x}\in\mathcal{V}\)
\item \(\inner{\bm{x}}{\bm{x}} = 0\)  if and only if \(\bm{x}=\bm{0}\)
\item \(\inner{\alpha\, \bm{x}}{\bm{y}} = \alpha\, \inner{\bm{x}}{\bm{y}}\)
\item \(\inner{\bm{x}}{\bm{y}+\bm{z}} = \inner{\bm{x}}{\bm{y}} +
        \inner{\bm{x}}{\bm{z}}\)
\item \(\inner{\bm{x}}{\bm{y}} = \inner{\bm{y}}{\bm{x}}\)
\end{enumerate}
\item The inner-product induces a norm: \(\len{x} = \sqrt{\inner{\bm{x}}{\bm{x}}}\)
\item For normal vectors (i.e. \(\bm{x}\in\mathbb{R}^{n\)\}) the standard inner product is the dot-product
\begin{align*}
    \langle \bm{x}, \bm{y} \rangle = \bm{x}^\tr\bm{y} = \sum_{i=1}^n x_i\,y_i
\end{align*}
\item We can define an inner product between functions
\begin{align*}
  \langle f, g \rangle = \int_{x\in\mathcal{I}} f(x)\,g(x)\,\dd x
\end{align*}
\item For matrices we can define the Frobenius inner-product
\begin{align*}
  \inner{\mat{A}}{\mat{B}}_{F} = \mathrm{Tr}\, \mat{A}^{\tr}\,\mat{B} = \sum_{i,j} A_{ij} \, B_{ij}
\end{align*}
which defines the Frobenius norm 
\begin{align*}
\len{\mat{A}}_{F} = \sqrt{\inner{\mat{A}}{\mat{A}}_{F}} = \sqrt{ \sum_{i,j} A_{ij}^{2}}
\end{align*}
The Frobenius norm is not a compatible norm.
\item A really important inequality is the \emph{Cauchy-Schwarz} inequality
     \begin{align*}
  \inner{\bm{x}}{\bm{y}}^2 \leq \inner{\bm{x}}{\bm{y}} \,
  \inner{\bm{y}}{\bm{y}} = \len{\bm{x}}^2\,\len{\bm{y}}^2\pause
\end{align*}
\item Inner products allow us to define the notion of similarity
\begin{align*}
    \langle \bm{x}, \bm{y} \rangle &= \|\bm{x}\| \, \|\bm{x}\| \, \cos(\theta) \\
    \langle f(x), g(x) \rangle &= \|f(x)\| \, \|g(x)\|\, \cos(\theta)
\end{align*}
\item \(\cos(\theta)\) can be seen as a measure of the correlation
between vectors (or functions)
\item Because of Cauchy-Schwarz \(\langle \bm{x}, \bm{y}
     \rangle/(\|\bm{x}\| \, \|\bm{y}\|)\) lies between -1 and 1 (so
that we can represent this quantity by the cosine of an angle)
\end{itemize}

\subsection{Coordinates or Basis Vectors}
\label{sec:org2f10509}
\begin{itemize}
\item Any set of vectors that span the entire vector space can be
considered a set of basis vectors or coordinates
\item If our bases are linearly independent then we have a set of
non-degenerate basis function where each vector is unique
\item The most convenient set of basis vectors are those where the
bases are normalised and orthogonal
\(\langle\bm{b}_i,\bm{b}_j\rangle=\delta_{ij}\)
\item \textbf{Basis Functions}
\begin{itemize}
\item For a function space we can consider a set of basis functions
\item A familiar set of functions define on the interval \([0,2\,\pi]\)
are the Fourier functions 
$$ \{1,\, \cos(\theta),\, \sin(\theta),\, \cos(2\,\theta),\,
       \sin(2\,\theta),\, \cdots\} $$
\item This basis set is orthogonal as for any two components \(\langle
       b_i(\theta),\,b_j(\theta)\rangle = \delta_{ij}\)
\item There are many orthogonal polynomials that have similar properties
\item Given an orthogonal set of functions \(\{b_i(\bm{x})\}\) we can decompose a function \(f(\bm{x})\)
as a (infinite) vector \(\bm{f}\) with components \(f_i = \langle f(\bm{x}),b_i(\bm{x})\rangle\)
\item This allows us to represent any function as a point in an infinite-dimensional space
\end{itemize}
\end{itemize}

\subsection{Operators}
\label{sec:orgccfa305}
\begin{itemize}
\item Operators transform elements of a vector space
\item Consider the transformation or operator \(\mathcal{T}: \mathcal{V} \rightarrow \mathcal{V}'\)
\item This says that \(\mathcal{T}\) maps some object
\(\bm{x}\in\mathcal{V}\) to an object \(\bm{y}=\mathcal{T}[\bm{x}]\)
in a new vector space \(\mathcal{V}'\)
\item \textbf{Linear Operators}
\begin{itemize}
\item Linear operators satisfy the two conditions
\begin{enumerate}
\item \(\mathcal{T}[a\,\bm{x}] = a\,\mathcal{T}[\bm{x}]\)
\item \(\mathcal{T}[\bm{x} + \bm{y}] = \mathcal{T}[\bm{x}] + \mathcal{T}[\bm{y}]\)
\end{enumerate}
\item Linear operators are really important because we can understand them
\item For normal vectors the most general linear operation is
\begin{align*}
  \mathcal{T}[\bm{x}] = \mat{M}\,\bm{x}
\end{align*}
where \(\mat{M}\) is a matrix
\item For functions the most general linear operator is a kernel function
\begin{align*}
   g(\bm{x}) = \mathcal{T}[f(\bm{x})] = \int
   K(\bm{x},\bm{y})\, f(\bm{y}) \, \dd \bm{y}
\end{align*}
\begin{itemize}
\item Kernels appear in SVMs, SVRs, kernel-PCA, Gaussian Processes
\end{itemize}
\end{itemize}
\item Often we are interested in operators that map vectors in a vector space to new
vectors in the same vector space
\begin{itemize}
\item \(\mathcal{T}: \mathcal{V}\rightarrow\mathcal{V}\)
\item The most general linear mapping for normal vectors that does this are square matrices
\end{itemize}
\end{itemize}
\subsection{Matrices and Mappings}
\label{sec:orgb50cb5a}
\begin{itemize}
\item Matrices, \(\mat{M}\) are linear maps from one point \(\bm{x}\) to
another \(\bm{y}=\mat{M}\bm{x}\)
\item The product of a matrix \(\mat{C}  = \mat{A}\mat{B}\) corresponds
to applying the mapping \(\mat{B}\) followed by \(\mat{A}\)
\item For most matrices \(\mat{A}\mat{B} \neq \mat{B}\mat{A}\) (we say
matrix multiplication does not commute)
\item There are pairs of matrices that do commute, but they need to
share a special structure for this to happen
\item Matrix multiplication is associative.  That is, \((\mat{A}\,\mat{B})\mat{C} = \mat{A} (\mat{B}\, \mat{C})\)
\end{itemize}
\end{document}
