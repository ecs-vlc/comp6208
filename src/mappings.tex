%Master File:lectures.tex

\lesson{Understand Mappings}
\vspace{-2cm}
\begin{center}
  \includegraphics[width=0.9\linewidth]{inverseProblem}
\end{center}
\keywords{Mappings, Linear Maps, Solving Linear Systems}
%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\Outline}{%
\begin{slide}
\section[1]{Outline}

\begin{minipage}{12cm}
  \begin{enumerate}\squeeze
    \outlineitem{Mappings}{mappings}
    \outlineitem{Linear Maps}{linearmaps}
  \end{enumerate}
\end{minipage}\hfill
\begin{minipage}{10cm}
  \includegraphics[height=10cm]{ill_conditioned_map}
\end{minipage}
\end{slide}
\addtocounter{outlineitem}{1}
}

\setcounter{outlineitem}{1}
\Outline
\toptarget{firstoutline}
%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Transforming Data}

\begin{PauseHighLight}
  \begin{itemize}
  \item In the last lecture we spent time developing a sophisticate view
    of vector spaces and operators\pause
  \item At a mathematical level machine learning can be viewed as
    performing an inverse mapping
    \begin{center}
      \includegraphics[width=12cm]{inverseProblem}\pause
    \end{center}
  \item Although our mappings are not necessarily linear in either
    direction we learn a lot by understanding linear operators\pause
  \end{itemize}
\end{PauseHighLight}



\end{slide}
%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Inverse Problems}

\begin{PauseHighLight}
  \begin{itemize}
  \item Given $m$ observations $\{(\bm{x}_k, y_k)| k=1,\ldots,m\}$ and
    $p$ unknown $\bm{w}=(w_1,w_2, \ldots w_p)$ such that
    $\bm{x}_k^\tr \bm{w} = y_k$ then to find $\bm{w}$\pause
\item Define the \textit{design matrix} as the matrix of feature vectors
  \begin{displaymath}
    \mat{X} = 
    \begin{pmatrix}
      \bm{x}_1^\tr \\ \bm{x}_2^\tr \\ \hdots \\ \bm{x}_m^\tr
    \end{pmatrix} =
    \begin{pmatrix}
      x_{11} & x_{12} & \cdots & x_{1p}\\
      x_{21} & x_{22} & \cdots & x_{2p}\\
      \vdots & \vdots & \ddots & \vdots \\
      x_{m1} & x_{m2} & \cdots & x_{mp}
    \end{pmatrix}\pause
  \end{displaymath}
\item and the target vector $\bm{y} = (y_1, y_2, \cdots, y_m)^\tr$\pause
\item Then if $m=p$ we have $\bm{y} = \mat{X} \bm{w}$ or $\bm{w} =
  \mat{X}^{-1} \bm{y}$\pause
\end{itemize}

\end{PauseHighLight}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Linear Regression}

\pb
\begin{itemize}\squeeze
\item $\bm{x}_k^\tr \bm{w}$ depends on distance from separating\pauseh\pauselevel{=1}
\vspace*{-1cm}
\begin{center}
\multipdf[height=9.5cm]{linearRegression}\pause\\
\vspace*{-0.5cm}
\end{center}
\item If $m>p$ then $\mat{X}$ isn't square so doesn't have an inverse\pauseh
\item Worse unless the data is accurate $\bm{y} \approx \mat{X}
  \bm{w}\Rightarrow$ no ``solution''\pauseh
\item Problem solved by Gauss to predict the orbit of the asteroid Ceres\pauseh
\end{itemize}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-1.5]{Linear Least Squares}

\begin{PauseHighLight}

\begin{itemize}\squeeze
\item The error of input pattern $\bm{x}_k$ is
  \begin{displaymath}
    \epsilon_k  = \bm{x}_k^\tr \bm{w}-y_k\pause
  \end{displaymath}
\item The squared error
  \begin{displaymath}
    E(\bm{w}|\mathcal{D}) = 
    \sum_{k=1}^m\left(\bm{x}_k^\tr \bm{w}-y_k\right)^2
    = \sum_{k=1}^m \epsilon_k^2 = \| \bm{\epsilon} \|^2 \pause
  \end{displaymath}
\item We can define the error vector
  \begin{displaymath}
    \bm{\epsilon} = \mat{X} \bm{w} - \bm{y}
  \end{displaymath}
(note that $\epsilon_k = \bm{x}_k^\tr \bm{w} -y_k$)\pause
\item Minimising this error is known as the least squares problem\pause
\end{itemize}

\end{PauseHighLight}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Finding a Minimum}

\begin{PauseHighLight}
  \begin{itemize}
  \item The minima of a one dimensional function, $f(x)$, are given by
    $f'(x)=0$
    \begin{center}
      \includegraphics[width=0.5\linewidth]{gradient}\pause
    \end{center}
  \item The minima of an $n$-dimensions function $f(\bm{x})$ are
    given by the set of equations
    \begin{align*}
      \frac{\partial f(\bm{x})}{\partial x_i}=0 
      \quad \forall i=1,\ldots n\pause
    \end{align*}
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
  \section[-1]{Gradients}

  \begin{PauseHighLight}

    \begin{itemize}
    \item The \emph{grad} operator $\grad$ is the gradient operator in
      high dimensions
      \begin{displaymath}
        \grad f(\bm{x}) = 
        \begin{pmatrix}
          \frac{\partial f(\bm{x})}{\partial x_1} \\
          \frac{\partial f(\bm{x})}{\partial x_2} \\
          \vdots \\
          \frac{\partial f(\bm{x})}{\partial x_n}
        \end{pmatrix}\pause
      \end{displaymath}
    \item The partial derivatives (curly d's)
      \begin{displaymath}
        \frac{\partial f(\bm{x})}{\partial x_i}
      \end{displaymath}
      means differentiate with respect to $x_i$ treating all other components
      $x_j$ as constants\pause
    \end{itemize}

  \end{PauseHighLight}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Least Squares Solution}

\begin{PauseHighLight}

\begin{itemize}\squeeze
\item The least squared solution is give by
  \begin{align*}
    \grad E(\bm{w}|\mathcal{D}) &= \grad \|\bm{\epsilon}\|^2 \pause
    = \grad \| \mat{X} \bm{w} - \bm{y}\|^2 \pause \\
    &= \grad \left(\bm{w}^\tr
      \mat{X}^\tr\mat{X} \, \bm{w} -  2 \bm{w}^\tr\mat{X}^\tr \bm{y} +
      \bm{y}^\tr  \bm{y}\right)\pause\\
    &= 2 \left( \mat{X}^\tr\mat{X}\, \bm{w} - \mat{X}^\tr
      \bm{y}\right)\pause = 0 \pause
  \end{align*}
\item Or
  \begin{displaymath}
    \bm{w} = \left(\mat{X}^\tr\mat{X}\right)^{-1} \mat{X}^\tr \bm{y}\pause
    = \mat{X}^{+} \bm{y}
  \end{displaymath}
\item $\mat{X}^{+} = \left(\mat{X}^\tr\mat{X}\right)^{-1} \mat{X}^\tr$ is
  known as the pseudo inverse\pause
\item For non-square matrices Matlab uses the pseudo inverse so in
  Matlab we can write
  \begin{matlab}
    w = X\y
  \end{matlab}\pause
\end{itemize}

\end{PauseHighLight}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-1]{Missing Bits of the Mathematics}

\begin{PauseHighLight}
  \begin{itemize}
  \item Note that $\len{\bm{a}}^2 = \bm{a}^\tr \bm{a} = \sum\limits_i a_i^2$\pause
    \begin{align*}
      \| \mat{X} \bm{w} - \bm{y}\|^2 &= (\mat{X} \bm{w} -
      \bm{y})^\tr(\mat{X} \bm{w} - \bm{y})\pause = (\bm{w}^\tr \mat{X}
                                       ^\tr - \bm{y}^\tr)(\mat{X}
                                       \bm{w} - \bm{y})\pauseb\\
      &=\bm{w}^\tr
      \mat{X}^\tr\mat{X} \, \bm{w} -  2 \bm{w}^\tr\mat{X}^\tr \bm{y} +
      \bm{y}^\tr  \bm{y}\pauseb
    \end{align*}
  \item Where we have used $\bm{w}^\tr\mat{X}^\tr \bm{y} =
    \bm{y}^\tr\mat{X} \bm{w}$\pause{}, $\sum\limits_{i,j} w_i X_{ji} y_j
    = \sum\limits_{i,j} y_i X_{ij} w_j$\pauseb
  \item Also $\grad \bm{w}^\tr \mat{M}\, \bm{w} = \mat{M}\,\bm{w} + \mat{M}^\tr\, \bm{w}$\pause
  \item If $\mat{M}=\mat{M}^\tr$ (i.e.{} $\mat{M}$ is symmetric) then
    $\grad \bm{w}^\tr \mat{M}\, \bm{w} = 2\,\mat{M}\,\bm{w}$\pause
  \item $(\mat{X}^\tr\mat{X})^\tr = \mat{X}^\tr\mat{X}$ so that
    $\mat{X}^\tr\mat{X}$ is symmetric\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Computing Gradients}

\begin{PauseHighLight}
  \begin{itemize}
  \item To understand gradients we sometimes need to go back to
    components\pause
    \begin{align*}
      \grad \bm{w}^\tr \mat{M}\, \bm{w}\pause &=
      \begin{pmatrix}
        \pd{\ }{w_1} \\  \pd{\ }{w_2} \\  \pd{\ }{w_3} \\ \vdots
      \end{pmatrix}
      \sum_{i,j} w_i M_{ij} w_j \pauseb =
      \begin{pmatrix}
        \sum_j M_{1j} w_j + \sum_i w_i M_{i1} \\
        \sum_j M_{2j} w_j + \sum_i w_i M_{i2} \\
        \sum_j M_{3j} w_j + \sum_i w_i M_{i3} \\ \vdots
      \end{pmatrix} \pauseb\\
      &= \mat{M}\, \bm{w} + \mat{M}^\tr\, \bm{w}\pauseb
    \end{align*}
  \item It is tedious to compute these things component-wise, but when
    you need to understand what is going on then go back to the basics\pauseb
  \end{itemize}
\end{PauseHighLight}
  

\end{slide}



%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%
\Outline % Linear Maps
%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Solving Inverse Problems}

\begin{PauseHighLight}
  \begin{itemize}
  \item Gauss showed us how to solve \emph{over-constrained} problems
    (we have more observations than parameters)\pause
  \item We seek a solution which isn't necessarily exact but minimises
    an error\pause
  \item But, what if we have more parameters than observations\pause
  \item That is, we are \emph{under-constrained}\pause
  \item Note that in some directions you might be over-constrained and
    in other directions under-constrained\pause
  \item This is very typical of most machine learning problems\pauseb
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Under Constrained Systems}

\begin{PauseHighLight}
  \begin{itemize}
  \item If we have less data-points than parameters then there will be
    multiple solutions
  \begin{center}
    \includegraphics[height=12cm]{curves}\pause
  \end{center}
  \end{itemize}
\end{PauseHighLight}

\end{slide}


%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{What is the Inverse?}

\pb
  \begin{itemize}
  \item Many points can map to the same points\pause
    \begin{center}
      \includegraphics[width=0.75\linewidth]{underConstrained0}\mypl{1}
      \llap{\includegraphics[width=0.75\linewidth]{underConstrained1}}\mypl{2}
    \end{center}
  \end{itemize}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-1]{Under-constrained Systems}

\begin{PauseHighLight}
  \begin{itemize}
  \item The system is \emph{under-constrained}\pause
  \item We have more unknowns than equations\pause
  \item The inverse is not unique\pause
  \item Solving the inverse problem ($\bm{w} =
    \left(\mat{X}^\tr\mat{X}\right)^{-1} \mat{X}^\tr \bm{y}$) is said 
    to be \emph{ill-posed}\pause
  \item The inverse $\left(\mat{X}^\tr\mat{X}\right)^{-1}$ doesn't
    exist\pause 
  \item If we have a complicated learning machine and not sufficient
    data we often end with an ill-posed inverse problem (there are lots
    of sets of parameters that explain the data)\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}


%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
  \section{Ill-Conditions}

  \begin{PauseHighLight}
    \begin{itemize}
    \item Singular matrices are rare (although they occur when we don't
      have enough data), but matrices that are close to
      being singular are common\pause
    \item If a matrix is close to singular it is ill-conditioned\pause
    \item Ill-conditioned matrices have some small eigenvalues\pause
    \item All points get contracted towards a plane\pause
    \item Large matrices are very often ill conditioned\pause
    \end{itemize}
  \end{PauseHighLight}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
  \section[-1]{Ill-Conditioned Matrices}

  \begin{PauseHighLight}

    \begin{center}
      \includegraphics[height=15cm]{ill_conditioned_map}
    \end{center}

  \end{PauseHighLight}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Ill-Conditioning in ML}

\begin{PauseHighLight}
  \begin{itemize}
  \item Ill-conditioning in machine learning occurs when a very small
    change in the learning data causes a large change in the predictions
    of the learning machine\pause
  \item In linear regression the matrix $\mat{X}^\tr\mat{X}$ is
    ill-conditioned when we have as many data points as parameters\pause
  \item Much of machine learning is concerned with making learning
    machines better conditioned\pause
  \item Adding regularisers is one approach to achieve this\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}


%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Summary}

\begin{PauseHighLight}
  \begin{itemize}
  \item Linear mappings are commonly used in machine learning algorithms
    such as regression\pause
  \item We will often meet the pseudo-inverse involving inverting
    $\mat{X}^\tr\,\mat{X}$\pause 
  \item They can be inherently unstable to noise in the inputs\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}


%%% Local Variables:
%%% TeX-master: "lectures"
%%% End:
