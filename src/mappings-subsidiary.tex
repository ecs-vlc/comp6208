% Created 2024-02-09 Fri 16:28
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[a4paper,margin=20mm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{bm}
\usepackage{minted}
\usemintedstyle{emacs}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beraserif}
\usepackage[scaled]{berasans}
\usepackage[scaled]{beramono}
\newcommand{\tr}{\textsf{T}}
\newcommand{\grad}{\bm{\nabla}}
\newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
\newcommand{\Prob}[2][]{\mathbb{P}_{#1\!}\left[ #2 \right]}
\newcommand{\logg}[1]{\log\!\left( #1 \right)}
\newcommand{\pred}[1]{\left\llbracket { \small #1} \right\rrbracket}
\newcommand{\e}[1]{{\rm e}^{#1}}
\newcommand{\dd}{\mathrm{d}}
\DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
\newcommand{\normal}[2]{\mathcal{N}\!\left(#1 \big| #2 \right)}
\newcounter{eqCounter}
\setcounter{eqCounter}{0}
\newcommand{\explanation}{\setcounter{eqCounter}{0}\renewcommand{\labelenumi}{(\arabic{enumi})}}
\newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\arabic{eqCounter})}}{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\Dist}[2][Binom]{\mathrm{#1}\left( \strut {#2} \right)}
\author{Adam Prügel-Bennett}
\date{\today}
\title{Advanced Machine Learning Subsidary Notes\\\medskip
\large Lecture 9: Understand Mappings}
\hypersetup{
 pdfauthor={Adam Prügel-Bennett},
 pdftitle={Advanced Machine Learning Subsidary Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.3)}, 
 pdflang={English}}
\begin{document}

\maketitle


\section{Keywords}
\label{sec:orge0104e5}
\begin{itemize}
\item Mappings, Eigenvectors
\end{itemize}

\section{Main Points}
\label{sec:org4ab381b}

\subsection{Inverse Problems}
\label{sec:org847b268}
\begin{itemize}
\item Much of machine learning can be viewed as solving an inverse problem
\item We collect data about the world by performing a series of measurements
\item Our task is to infer properties of the world from the data
\end{itemize}

\subsection{Over-Constrained Problems}
\label{sec:org445cb81}
\begin{itemize}
\item We can have contradictory data that our model cannot explain
\item This may arise because
\begin{itemize}
\item We have errors in the data
\item Our data contains insufficient information
\item Our model is too simple
\end{itemize}
\item If we have more training data than free parameters this is likely to occur
\item We typically solve this by introducing a loss function we minimise
\item A classic example is to minimise the squared error
\end{itemize}

\subsection{Under-Constrained Problems}
\label{sec:org7051429}
\begin{itemize}
\item We can also be in a situation when many models (learning
machines) explain the data
\item This will typically happen when we have more free parameters than data
\item Here we have to choose a particular model
\item To do this requires (implicitly or explicitly) making additional
assumptions
\item For high-dimensional inputs we can be over-constrained
in some directions and under-constrained in others
\end{itemize}

\subsection{Ill-Conditioning}
\label{sec:org94e1006}
\begin{itemize}
\item Even when we are not under-constrained our inverse can be very sensitive to the data
\item That is small errors can be strongly magnified
\item Ill-condition leads to high variance in the bias-variance sense and hence poor generalisation
\end{itemize}

\subsection{Linear Regression}
\label{sec:org5dabf46}
\begin{itemize}
\item In linear regression we try to fit a linear model \(y_i = \bm{x}_i^\tr\,\bm{w}\)
(or in matrix form \(\bm{y} = \mat{X}\,\bm{w}\))
\item We use a squared error (so can cope with conflicting constraints)
\item If we have more training examples than parameters the solution is given by the pseudo-inverse
\(\bm{w} = (\mat{X}^\tr \mat{X})^{-1} \mat{X}^\tr \bm{y}\)
\item It we have less training examples than parameters (or we are
unlucky in that the training examples don't span the full space)
then the problem is under-constrained and there are an infinity
of solutions
\item Even when we have more training examples than parameters the
problem can be ill-conditioned
\end{itemize}


\section{Exercises}
\label{sec:orgeb01b4e}

\subsection{Linear Regression}
\label{sec:org598fb56}
\begin{itemize}
\item Derive the formula for the weight vector in linear regression
\end{itemize}

\section{Experiments}
\label{sec:orgace939c}

\subsection{Eigensystems}
\label{sec:orga9d5940}
\begin{itemize}
\item In either Matlab/Octave or python generate random matrices and check
the matrix identities
\end{itemize}

\begin{minted}[]{matlab}
X = randn(5,4)  % generate a mock designer matrix with 5 inputs of length 4 
M = X'*X        % compute a symmetrix matrix
[V.L] = eig(M)  % compute eigenvalues
V*L*V'          % should be identical to M
V*V'            % should be the identity matrix (up to rounding precision)
V'*V            % should be the identity matrix (up to rounding precision)
x = randn(4,1)  % generate a random column matrix of length 4
y = randn(4,1)  % generate another random column matrix of length 4
xp = V*x        % apply V to x
yp = V*y        % apply V to y
norm(x)         % compute Euclidean norm of x
norm(xp)        % should be the same as Euclidean nor of xp
x'*y            % compute inner product of x and y
xp'*yp'         % compute inner produce of xp and yp (should be the same as above)

Z = rand(4,5)   % consider a designer matrix where we would have more unknowns the examples
W = Z'*Z        % compute a covariance type matrix (except we don't subtract the mean
eig(W)          % compute eigenvalues (one should be 0 up to machine precision)
\end{minted}
\end{document}
