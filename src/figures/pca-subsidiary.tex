% Created 2021-01-28 Thu 17:17
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[a4paper,margin=20mm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{bm}
\usepackage{minted}
\usemintedstyle{emacs}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beraserif}
\usepackage[scaled]{berasans}
\usepackage[scaled]{beramono}
\newcommand{\tr}{\textsf{T}}
\newcommand{\grad}{\bm{\nabla}}
\newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
\newcommand{\Prob}[2][]{\mathbb{P}_{#1\!}\left[ #2 \right]}
\newcommand{\logg}[1]{\log\!\left( #1 \right)}
\newcommand{\pred}[1]{\left\llbracket { \small #1} \right\rrbracket}
\newcommand{\e}[1]{{\rm e}^{#1}}
\newcommand{\dd}{\mathrm{d}}
\DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
\newcommand{\normal}[2]{\mathcal{N}\!\left(#1 \big| #2 \right)}
\newcounter{eqCounter}
\setcounter{eqCounter}{0}
\newcommand{\explanation}{\setcounter{eqCounter}{0}\renewcommand{\labelenumi}{(\arabic{enumi})}}
\newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\arabic{eqCounter})}}{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\Dist}[2][Binom]{\mathrm{#1}\left( \strut {#2} \right)}
\author{Adam Prügel-Bennett}
\date{\today}
\title{Advanced Machine Learning Subsidary Notes\\\medskip
\large Lecture 7: Principal Component Analysis (PCA)}
\hypersetup{
 pdfauthor={Adam Prügel-Bennett},
 pdftitle={Advanced Machine Learning Subsidary Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle


\section{Keywords}
\label{sec:org92ce1fa}
\begin{itemize}
\item Covariance matrices, dimensionality reduction, PCA, Duality
\end{itemize}

\section{Main Points}
\label{sec:orgbc9c8d3}

\subsection{PCA}
\label{sec:orga4fee7f}
\begin{itemize}
\item This is revision as you should have all seen this in foundations of ML
\item The covariance matrix is defined as 
$$ \mat{C}= \frac{1}{m+1} \sum_{i=1}^m
     (\bm{x}_i-\hat{\bm{\mu}})\,(\bm{x}_i-\hat{\bm{\mu}})^\tr $$
\item Defining the matrix \(\mat{X}\) as
\begin{align*}
  \mat{X} = \frac{1}{\sqrt{m-1}}\left( \strut \bm{x}_1-\bm{\mu},
  \bm{x}_2-\bm{\mu}, \cdots \bm{x}_m-\bm{\mu} \right)
\end{align*}
then \(\mat{C} = \mat{X} \, \mat{X}^\tr\)
\item The \emph{principal components} are the eigenvectors of the covariance
matrix with the largest eigenvalues
\item We can reduce the dimensionality of the inputs by projecting into
the subspace spanned by the principal components
\item We can reconstruct a vector from its principal component projection
\begin{align*}
  \hat{\bm{x}} = \sum_i z_i \, \bm{v}_i
\end{align*}
\begin{itemize}
\item \(\bm{v}_i\) are the principal components (eigenvectors of the
covariance matrix with largest eigenvalues)
\item \(z_i = \bm{v}_i^\tr \bm{x}\) are the values of the new features
\item the sum is over the principal  components that we
\end{itemize}
\item The expected squared error reconstruction loss
\(\mathbb{E}\left[(\hat{\bm{x}}-\bm{x})^2\right]\) is equal to the
sum of the eigenvalues we ignore
\end{itemize}

\subsection{Duality}
\label{sec:org322d87e}
\begin{itemize}
\item We can define the dual matrix, \(\mat{D}= \mat{X}^\tr\mat{X}\) with
components \(D_{kl} = (\bm{x}_k-\bm{\mu})^\tr(\bm{x}_k-\bm{\mu})\)
\item If \(\bm{u}_i\) is and eigenvector of \(\mat{D}\) with eigenvalue
\(\lambda_i\) then \(\bm{v}_i = \mat{D} \, \bm{u}_i\) is an
eigenvector of \(\mat{C}\) with the same eigenvalue
\item Matrix \(\mat{C}\) and \(\mat{D}\) have exactly the same non-zero
eigenvalues
\item If we have more features than training example it is more
efficient to work with \(\mat{D}\) than \(\mat{C}\)
\item Note in this case the training examples will not span the feature
space.  \(\mat{D}\) describes the fluctuations in the space spanned
by the examples
\end{itemize}

\section{Exercises}
\label{sec:org762cc53}

\subsection{Duality}
\label{sec:org054e483}
\begin{itemize}
\item Show that if \(\bm{u}_i\) is and eigenvector of \(\mat{D}\) with eigenvalue
\(\lambda_i\) then \(\bm{v}_i = \mat{D} \, \bm{u}_i\) is an
eigenvector of \(\mat{C}\) with the same eigenvalue
\item Answer in the lecture notes
\end{itemize}

\section{Experiments}
\label{sec:org81891da}

\subsection{Duality}
\label{sec:org18bbe71}
\begin{itemize}
\item Using Matlab/Octave of python illustrate that the dual matrix and
covariance matrix have the same eigenvalues
\end{itemize}
\begin{minted}[]{matlab}
X = randn(5,3)   % construct a random matrix
C = X*X'         % compute a sort of covariance matrix (haven't bothered removing mean
D = X'*X         % compute dual
[V,LC] = eig(C)  % compute eigensystem of C
[U,LD] = eig(D)  % compute eigensystem of D should have the same non-zero eigenvalues
u1 = X*U(:,1)    % left multiply and eigenvector or D by X
u1/norm(u1)      % normalise above should be the same as V(:,3) (could be V(:,4) or V(:,5))
\end{minted}
\end{document}
