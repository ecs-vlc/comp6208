% Created 2021-01-28 Thu 17:12
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[a4paper,margin=20mm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{bm}
\usepackage{minted}
\usemintedstyle{emacs}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beraserif}
\usepackage[scaled]{berasans}
\usepackage[scaled]{beramono}
\newcommand{\tr}{\textsf{T}}
\newcommand{\grad}{\bm{\nabla}}
\newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
\newcommand{\Prob}[2][]{\mathbb{P}_{#1\!}\left[ #2 \right]}
\newcommand{\logg}[1]{\log\!\left( #1 \right)}
\newcommand{\pred}[1]{\left\llbracket { \small #1} \right\rrbracket}
\newcommand{\e}[1]{{\rm e}^{#1}}
\newcommand{\dd}{\mathrm{d}}
\DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
\newcommand{\normal}[2]{\mathcal{N}\!\left(#1 \big| #2 \right)}
\newcounter{eqCounter}
\setcounter{eqCounter}{0}
\newcommand{\explanation}{\setcounter{eqCounter}{0}\renewcommand{\labelenumi}{(\arabic{enumi})}}
\newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\arabic{eqCounter})}}{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\Dist}[2][Binom]{\mathrm{#1}\left( \strut {#2} \right)}
\author{Adam Prügel-Bennett}
\date{\today}
\title{Advanced Machine Learning Subsidary Notes\\\medskip
\large Lecture 2: Over-Fitting}
\hypersetup{
 pdfauthor={Adam Prügel-Bennett},
 pdftitle={Advanced Machine Learning Subsidary Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle


\section{Keywords}
\label{sec:org7878079}
\begin{itemize}
\item Overfitting, regularisation, feature selection
\end{itemize}

\section{Main Points}
\label{sec:org0fa874f}

\begin{itemize}
\item Over-fitting
\begin{itemize}
\item Over-fitting is when we learn \textbf{spurious rules} that explain the
training set
\item If we use an infinitely flexible machine then we can't
generalise at all
\end{itemize}
\item Controlling Complexity
\begin{itemize}
\item More training example improves generalisation by allowing us to
eliminate spurious rules
\item To achieve good generalisation performance machine must match
the underlying structure of the problem
\item By preprocessing or careful feature engineering you can sometimes
make the learning task easier
\item Deep learning works because
\begin{itemize}
\item It uses a lot of training data
\item They use convolutions that
\begin{itemize}
\item have many fewer parameter to learn than fully connected layers
\item respect translation invariance
\item find local features (at different scales through the network)
\end{itemize}
\end{itemize}
\item We usually don't know the structure of the inputs (they are too
high dimensional)
\begin{itemize}
\item Try different machines to see what fits
\item Fine-tune the models (adjusting hyper-parameters)
\item Feature Engineering very often help
\begin{itemize}
\item Feature selection
\item Using PCA or clustering
\item Normalise your input features
\end{itemize}
\item Need a validation set to do this
\item Beware you are very likely to over-estimate your generalisation error
if you use your test set  to select your model
\item Can use \(K\)-fold cross-validation to get a better estimate of
generalisation error
\end{itemize}
\end{itemize}
\item Regularisation
\begin{itemize}
\item Adding regularisation terms can help choose a simpler model
\item \(L_2\) regularisers punish large weights making the fitting
function smoother
\item \(L_1\) regularisers can do automatic feature selection
\item Can do subtle forms of regularisation such as choosing a maximum
margin solution
\end{itemize}
\end{itemize}

\section{Exercises}
\label{sec:orgf4f8de1}

\subsection{Estimating the error in the mean}
\label{sec:org5db03f0}

\begin{itemize}
\item Consider a binary classification problem
\item Suppose we measure the accuracy on a validation set of size \(n=10\,000\)
\item If we correctly predict \(8\,000\) of the validation set what is
\begin{enumerate}
\item an estimate of the mean accuracy?
\item the accuracy of this estimate?
\end{enumerate}
\item State you assumptions
\item What would the answer be if \(n=100\) and we got 80 correct on 
the validation set?
\item Answer at the end
\end{itemize}


\section{Experiments}
\label{sec:org4b77ae2}

\subsection{Overfitting Game}
\label{sec:org0e2d341}
\begin{itemize}
\item Write your own simulation (see lecture notes)
\item Generate \(n\) random numbers \(X_i\sim U(0,3)\) (you can try different distributions)
\item Add some noise to generate \(Y_i = X_i + Z_i\) where \(Z_i\sim\mathcal{N}(0,1)\)
\item Choose \(i^*\) with the largest value of \(Y_i\)
\item Compute \(\Delta = Z_{i^*}= Y_{i^*}-X_{i^*}\)
\item Repeat many times and plot a histogram
\end{itemize}

\subsection{Cross-validation}
\label{sec:orgfb6bb14}
\begin{itemize}
\item Try using cross validation
\end{itemize}

\begin{minted}[]{python}
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn import svm

X, y = datasets.load_iris(return_X_y=True)
X.shape, y.shape

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, random_state=0)

X_train.shape, y_train.shape

X_test.shape, y_test.shape


clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
clf.score(X_test, y_test)
\end{minted}

\subsection{Project}
\label{sec:org88dcc73}

\begin{itemize}
\item There are a lot of practical tips that you can use in your project
\begin{itemize}
\item Try them out and see what works and what doesn't
\item Many modifications won't make much difference, but occasionally one
will make a lot of difference
\item Be very wary of estimating your generalisation performance
\begin{itemize}
\item You are measuring on a finite validation set so you will have errors
\item Once you've used your validation set for selecting models,
your likely to over-estimate your generalisation error
\end{itemize}
\end{itemize}
\end{itemize}


\section{Answers}
\label{sec:org508ace0}

\subsection{Estimating the error in the mean}
\label{sec:org0766564}

\begin{itemize}
\item We assume that the examples in the validation set are independent so
the number of successes, \(k\), will be binomially distributed
\begin{align*}
 \mathrm{Bin}(k|n,p) = \binom{n}{k} p^k\,(1-p)^{n-k}
\end{align*}
\item We don't know the success probability \(p\), but an unbiased estimate of
it is the number of observed success divided by \(n\), that is \(0.8\)
\item For a binomial the variance in \(k\) is given by \(n\,p\,(1-p)\) or 1600 
if \(p=0.8\) and \(n=10\,000\)
\item The typical size of the fluctuations in \(k\) would be \(\sqrt{n\,p\,(1-p)}=40\)
\item Our estimate of \(p\) would therefore typically have fluctuations
of size \(40/n =0.004\)
\item Thus our estimated success rate is \(0.8\pm0.004\) or 80\% with an error 
of about half a percent
\item This is small because we used a large validation set.  If \(n=100\) and
we got 80 successes then we would have an error of \(0.8\pm0.04\) so we have
an error of 4\%.
\item Note that by choosing the best of 10 machines we might expect to 
over-estimate the performance by around 5\% in this case
\end{itemize}
\end{document}
