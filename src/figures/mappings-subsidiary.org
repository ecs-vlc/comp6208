#+TITLE: Advanced Machine Learning Subsidary Notes
#+SUBTITLE: Lecture 6: Understand Mappings


* Keywords
  * Mappings, Eigenvectors

* Main Points

** Inverse Problems
   * Much of machine learning can be viewed as solving an inverse problem
   * We collect data about the world by performing a series of measurements
   * Our task is to infer properties of the world from the data

** Over-Constrained Problems
   * We can have contradictory data that our model cannot explain
   * This may arise because
     * We have errors in the data
     * Our data contains insufficient information
     * Our model is too simple
   * If we have more training data than free parameters this is likely to occur
   * We typically solve this by introducing a loss function we minimise
   * A classic example is to minimise the squared error

** Under-Constrained Problems
   * We can also be in a situation when many models (learning
     machines) explain the data
   * This will typically happen when we have more free parameters than data
   * Here we have to choose a particular model
   * To do this requires (implicitly or explicitly) making additional
     assumptions
   * For high-dimensional inputs we can be over-constrained
     in some directions and under-constrained in others

** Ill-Conditioning
   * Even when we are not under-constrained our inverse can be very sensitive to the data
   * That is small errors can be strongly magnified
   * Ill-condition leads to high variance in the bias-variance sense and hence poor generalisation

** Linear Regression
   * In linear regression we try to fit a linear model $y_i = \bm{x}_i^\tr\,\bm{w}$
     (or in matrix form $\bm{y} = \mat{X}\,\bm{w}$)
   * We use a squared error (so can cope with conflicting constraints)
   * If we have more training examples than parameters the solution is given by the pseudo-inverse
     $\bm{w} = (\mat{X}^\tr \mat{X})^{-1} \mat{X}^\tr \bm{y}$
   * It we have less training examples than parameters (or we are
     unlucky in that the training examples don't span the full space)
     then the problem is under-constrained and there are an infinity
     of solutions
   * Even when we have more training examples than parameters the
     problem can be ill-conditioned

** Eigen-Systems
   * We can understand ill-conditioning for linear regression by the
     eigen-decomposition of $\mat{M}=\mat{X}^\tr \mat{X}$
   * This should be revision
   * You know that an /eigenvector/, $\bm{v}$, satisfies $\mat{M}\,\bm{v}=\lambda\,\bm{v}$
   * For a symmetric matrix there are $n$ real orthogonal eigenvectors
   * You can prove they are orthogonal
   * *Orthogonal Matrices*
     * Putting the $n$ eigenvectors into a matrix $\mat{V}$ with
       columns $\bm{v}_i$ we obtain an orthogonal matrix
     * The defining property of an orthogonal matrix is
       $\mat{V}^\tr\mat{V} = \mat{V}\mat{V}^\tr=\mat{I}$
     * They correspond to rotations (with a possible reflection)
   * *Matrix Decomposition*
     * We can decompose a symmetric matrix, $\mat{M}$ as
       \begin{align*}
         \mat{M} = \mat{V}\,\mat{\Lambda}\,\mat{V}^\tr
       \end{align*}
     * Where $\mat{\Lambda}$ is a diagonal matrix of eigenvalues of $\mat{M}$ 
       (i.e. $\Lambda_{ii}=\lambda_i$)
     * $\mat{V}$ is the orthogonal matrix made up of the eigenvectors of $\mat{M}$
     * We can interpret the mapping of a symmetric matrix $\mat{M}$ as
       equivalent to
       1. a rotation defined by $\mat{V}^\tr$
       2. scaling of the $i^{th}$ component by $\lambda_i$ and
       3. a rotation backwards given by \mat{V}$
     * Equivalently if we work in a coordinate system defined by the
       eigenvectors of $\mat{V}$ (this forms an orthonormal basis set)
       then we just rescale in the directions $\bm{v}_i$ by $\lambda_i$
       * A symmetric matrix just squashes or expands in different
         orthogonal directions (this is what  the eigensystem captures)

   * *Inverse Matrices*
     * The inverse of a symmetric matrix $\mat{M}$ is given by
       \begin{align*}
         \mat{M} = \mat{V}\,\mat{\Lambda}^{-1}\,\mat{V}^\tr
       \end{align*}
     * Where $\mat{\Lambda}^{-1}$ is a diagonal matrix with elements $\Lambda_{ii}^{-1}=1/\lambda_i$
     * This is only defined if $\mat{M}$ if all the eigenvalues of
       $\mat{M}$ are non-zero  ($\mat{M}$ is said to be full rank)
     * If $\lambda_i$ is very small then $1/\lambda_i$ is large and in
       taking the inverse $\mat{M}^{-1}\bm{x}$ any component of $\bm{x}$
       in the direction $\bm{v}_i$ will get magnified by $1/\lambda_i$
     * For linear regression we invert $\mat{M} = \mat{X}^\tr\mat{X}$
       * in directions where the training examples don't vary much the
	 associated eigenvalue will be small and the inverse inherently
	 unstable


* Exercises

** Linear Regression
   * Derive the formula for the weight vector in linear regression

* Experiments

** Eigensystems
   * In either Matlab/Octave or python generate random matrices and check
     the matrix identities

#+BEGIN_SRC matlab
X = randn(5,4)  % generate a mock designer matrix with 5 inputs of length 4 
M = X'*X        % compute a symmetrix matrix
[V.L] = eig(M)  % compute eigenvalues
V*L*V'          % should be identical to M
V*V'            % should be the identity matrix (up to rounding precision)
V'*V            % should be the identity matrix (up to rounding precision)
x = randn(4,1)  % generate a random column matrix of length 4
y = randn(4,1)  % generate another random column matrix of length 4
xp = V*x        % apply V to x
yp = V*y        % apply V to y
norm(x)         % compute Euclidean norm of x
norm(xp)        % should be the same as Euclidean nor of xp
x'*y            % compute inner product of x and y
xp'*yp'         % compute inner produce of xp and yp (should be the same as above)

Z = rand(4,5)   % consider a designer matrix where we would have more unknowns the examples
W = Z'*Z        % compute a covariance type matrix (except we don't subtract the mean
eig(W)          % compute eigenvalues (one should be 0 up to machine precision)
#+END_SRC

* COMMENT [[file:mappings.pdf][PDF]] [[file:pdf/mappings_prn.pdf][Print]]
* COMMENT [[file:vectorSpaces-subsidiary.org][Previous]] [[file:pca-subsidiary.org][Next]]


* Options                                                  :ARCHIVE:noexport:
#+BEGIN_OPTIONS
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage[a4paper,margin=20mm]{geometry}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{stmaryrd}
#+LATEX_HEADER: \usepackage{bm}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usemintedstyle{emacs}
#+LaTeX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \usepackage[scaled]{beraserif}
#+LaTeX_HEADER: \usepackage[scaled]{berasans}
#+LaTeX_HEADER: \usepackage[scaled]{beramono}
#+LATEX_HEADER: \newcommand{\tr}{\textsf{T}}
#+LATEX_HEADER: \newcommand{\grad}{\bm{\nabla}}
#+LATEX_HEADER: \newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
#+LATEX_HEADER: \newcommand{\Prob}[2][]{\mathbb{P}_{#1\!}\left[ #2 \right]}
#+LATEX_HEADER: \newcommand{\logg}[1]{\log\!\left( #1 \right)}
#+LATEX_HEADER: \newcommand{\pred}[1]{\left\llbracket { \small #1} \right\rrbracket}
#+LATEX_HEADER: \newcommand{\e}[1]{{\rm e}^{#1}}
#+LATEX_HEADER: \newcommand{\dd}{\mathrm{d}}
#+LATEX_HEADER: \DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
#+LATEX_HEADER: \newcommand{\normal}[2]{\mathcal{N}\!\left(#1 \big| #2 \right)}
#+LATEX_HEADER: \newcounter{eqCounter}
#+LATEX_HEADER: \setcounter{eqCounter}{0}
#+LATEX_HEADER: \newcommand{\explanation}{\setcounter{eqCounter}{0}\renewcommand{\labelenumi}{(\arabic{enumi})}}
#+LATEX_HEADER: \newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\arabic{eqCounter})}}{#1}}
#+LATEX_HEADER: \newcommand{\argmax}{\mathop{\mathrm{argmax}}}
#+LATEX_HEADER: \newcommand{\Dist}[2][Binom]{\mathrm{#1}\left( \strut {#2} \right)}
#+END_OPTIONS

