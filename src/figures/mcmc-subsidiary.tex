% Created 2021-01-28 Thu 17:27
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[a4paper,margin=20mm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{bm}
\usepackage{minted}
\usemintedstyle{emacs}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beraserif}
\usepackage[scaled]{berasans}
\usepackage[scaled]{beramono}
\newcommand{\tr}{\textsf{T}}
\newcommand{\grad}{\bm{\nabla}}
\newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
\newcommand{\Prob}[2][]{\mathbb{P}_{#1\!}\left[ #2 \right]}
\newcommand{\logg}[1]{\log\!\left( #1 \right)}
\newcommand{\pred}[1]{\left\llbracket { \small #1} \right\rrbracket}
\newcommand{\e}[1]{{\rm e}^{#1}}
\newcommand{\dd}{\mathrm{d}}
\DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
\newcommand{\normal}[2]{\mathcal{N}\!\left(#1 \big| #2 \right)}
\newcounter{eqCounter}
\setcounter{eqCounter}{0}
\newcommand{\explanation}{\setcounter{eqCounter}{0}\renewcommand{\labelenumi}{(\arabic{enumi})}}
\newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\arabic{eqCounter})}}{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\Dist}[2][Binom]{\mathrm{#1}\left( \strut {#2} \right)}
\author{Adam Prügel-Bennett}
\date{\today}
\title{Advanced Machine Learning Subsidary Notes\\\medskip
\large Lecture 20: MCMC}
\hypersetup{
 pdfauthor={Adam Prügel-Bennett},
 pdftitle={Advanced Machine Learning Subsidary Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section{Keywords}
\label{sec:orgd660a44}
\begin{itemize}
\item Monte Carlo methods, MCMC, Variational Methods
\end{itemize}

\section{Main Points}
\label{sec:orga9dd7d2}

\subsection{Monte Carlo}
\label{sec:org74a24eb}
\begin{itemize}
\item Except in a few cases Bayesian inference is hard
\begin{itemize}
\item It is easy when we have a conjugate prior giving us a simple posterior
\item But, when we want to model complicated situations our
likelihood function usually doesn't have a conjugate prior
\item This means the posterior is not expressible as a simple
probability distribution
\item Furthermore it is usually impossible to compute the marginal
likelihood or evidence
$$ \Prob{\mathcal{D}} = \sum_{\bm{\Theta}}
       \Prob{\bm{\Theta}|\mathcal{D}} \, \Prob{\bm{\Theta}} $$
\item The sum or integral over variables \(\bm{\Theta}\) is often too
large to make this the feasible
\item You could try to obtain a histogram for the posterior, but if
\(\bm{\Theta}\) has many components the curse of dimensionality
makes this difficult
\end{itemize}
\item There is a simple answer which is to obtain samples from the posterior
\begin{itemize}
\item We can use this to estimate expectations of any function,
\(g(\bm{\Theta})\), of \(\bm{\Theta}\)
$$ \av[\bm{\Theta}|\mathcal{D}]{g(\bm{\Theta})} \approx
       \frac{1}{n} \sum_{i=1}^{n}   g(\bm{\Theta}_{i}) $$
\begin{itemize}
\item \(\bm{\Theta}_{i}\) are random deviates drawn from
the posterior \(\Prob{\bm{\Theta}_{i}|\mathcal{D}}\)
\item (random deviates or random variates are values drawn from a
probability distibution)
\item When the posterior distribution is well behaved (doesn't have a
thick tail) then the error in the approximation fall off as \(1/\sqrt{n}\)
\end{itemize}
\end{itemize}
\item The problem is you have to be able to sample random deviates
from \(P(\mathcal{D}|\bm{\Theta})\,f(\bm{\Theta})\)
\item For simple distributions there are two classic solutions to
generating random deviates
\begin{enumerate}
\item \emph{Transformation Methods}
\begin{itemize}
\item If we can compute the cumulative probability function
$$ F_{X}(x) = \int_{-\infty}^{x} f(t)\,\dd t $$
and invert \(F_{X}(x)\), then \(X=F^{-1}_{X}(U)\) will be a
random deviate with density \(f\), where \(U\)
is a uniform standard deviate between 0 and 1
\item This only works for a few classic distributions
\end{itemize}
\item \emph{Rejection Method}
\begin{itemize}
\item In the rejection methods we find another distribution
\(g_{Y}(y)\) that we can sample from that satisfies
$$ c\, g_{Y}(x) > f_{X}(x) $$
\item Then we generate a deviate \(Y\sim g_{Y}\) and accept it with
a probability \(f_{X}(Y)/(c\,g_{Y}(Y))\)
\item The expected rejection rate is \(c-1\)
\item This is a more general method, but is efficient only if we
have a good approximating function \(g_{Y}(y)\) (good in the
sense that \(c\) is not too large)
\item Particularly for multivariate (i.e. high dimensional) random
variables it is often very hard to find a good approximating
function
\end{itemize}
\end{enumerate}
\end{itemize}

\subsection{Markov-Chain Monte Carlo (MCMC)}
\label{sec:org0b746ce}
\begin{itemize}
\item Fortunately there is another method for generating random
deviates which is very general
\item This uses a Markov-Chain where we update an initial state of our system
\item If we choose the dynamics correctly eventually we will reach a
state that is correctly distributed
\item Suppose we have a set of states, \(\mathcal{S}\), and we want to choose a random
sample from a distribution \(\bm{\pi} = (\pi_{i}|i\in\mathcal{S}})\)
\begin{itemize}
\item \(\pi_{i}\) is the probability of being in state \(i\)---we get to
choose this (for Bayesian inference we will want to choose the
posterior probability)
\end{itemize}
\item Let \(M_{ij}\) be the transition probability from state \(j\) to
state \(i\)
\begin{itemize}
\item \(\sum_{i} M_{ij} =1\) (if we start any state, \(j\) say, we have
to end up in another state)
\item \(M_{ij}\geq0\) (it is a probability)
\item Matrices of non-negative elements whose rows sum to one are
known as \textbf{stochastic matrices}
\end{itemize}
\item If we choose \(M_{ij}\) so that
$$ M_{ij} \, \pi_{j} = M_{ji} \, \pi_{i} $$
\begin{itemize}
\item then after some time the probability of being in state \(i\) will
be \(\pi_{i}\) (there are some mild conditions on \(\mat{M}\) for
this to be true)
\item This condition is known as \textbf{detailed balance}
\item Note that if we sum the detailed balance equation then
$$ \sum_{j\in\mathcal{S}} M_{ij} \, \pi_{j} =
       \sum_{j\in\mathcal{S}} M_{ji} \, \pi_{i}  = \pi_{j}
       \label{eq:stochastic} $$
\begin{itemize}
\item since \(\sum_{j} M_{ji} = 1\)
\end{itemize}
\item Writing this in matrix form
$$ \mat{M} \bm{\pi} = \bm{\pi} $$
\begin{itemize}
\item That is, \(\bm{\pi}\) is an eigenvalue of \(\mat{M}\) with
eigenvalue 1
\end{itemize}
\item Because \(\mat{M}\) is a stochastic matrix all eigenvalues will
be less than or equal to 1
\begin{itemize}
\item To prove this let \(\bm{v}\) be an eigenvector of \(\mat{M}\)
with eigenvalue \(\lambda\)
$$  \sum_{j\in\mathcal{S}} M_{ij} \, v_{j} = \lambda \, v_{i} $$
taking the absolute value of both sides
$$ \biggl| \sum_{j\in\mathcal{S}} M_{ij} \, v_{j} \biggr|
         = |\lambda\, v_{i}| = |\lambda| \, |v_{i}| $$
\item But
$$ \biggl| \sum_{j\in\mathcal{S}} M_{ij} \, v_{j} \biggr|
	 \leq \sum_{j\in\mathcal{S}}\left| M_{ij} \, v_{j} \right|
	 = \sum_{j\in\mathcal{S}} M_{ij} \, |v_{j}| $$
where we have used \(M_{ij}\geq0\) (and essentially \(|a + b| \leq |a| + |b|\))
\item Thus
$$ \sum_{j\in\mathcal{S}} M_{ij} \, |v_{j}|  \geq |\lambda|
         \, |v_{i}| $$
\item Summing both sides with respect to \(i\)
\begin{align*}
\sum_{i\in\mathcal{S}} \sum_{j\in\mathcal{S}}
M_{ij}\, |v_{j}|  &\geq |\lambda| \,
\sum_{i\in\mathcal{S}} |v_{i}|  \\
\sum_{j\in\mathcal{S}} \sum_{i\in\mathcal{S}}
M_{ij}\, |v_{j}|  &\geq |\lambda| \,
\sum_{i\in\mathcal{S}} |v_{i}|  \\
\sum_{j\in\mathcal{S}}  |v_{j}| &\geq |\lambda| \,
\sum_{i\in\mathcal{S}} |v_{i}| 
\end{align*}
where we used \(\sum_{i\in\mathcal{S}}M_{ij} = 1\)
\item It follows that \(\lambda\leq 1\)
\item Let \(\bm{p}(t)\) be a vector whose elements \(p_{i}(t)\)
equal the probability of being in state \(i\) at time \(t\)
\item If the set of eigenvectors span the whole space, then any
starting vector \(\bm{p}(0)\) can be written in terms of the
eigenvectors 
$$ \bm{p}(0) = \sum_{i} c_{i}\,\bm{v}_{i} $$
(this is just a change of basis)
\item The probability vector after one step is given by
$$ \bm{p}(1) = \mat{M} \, \bm{p}(0) = \mat{M} \sum_{i} c_{i}\,\bm{v}_{i} 
	 =  \sum_{i} c_{i}\,\mat{M}\,\bm{v}_{i} 
         =  \sum_{i} c_{i}\,\lambda_{i}\,\bm{v}_{i} $$
\item After \(t\) steps
$$ \bm{p}(t) = \mat{M}^{t} \, \bm{x} = \sum_{i}
         c_{i}\,\lambda_{i}^{t}\,\bm{v}_{i} $$
\item If \(|\lambda|<1\) will then \(\lambda_{i}^{t}\) will shrink
exponentially fast so that
$$ \lim_{t\rightarrow\infty} \bm{p}(t) =
         \sum_{i:\lambda_{i}=0} c_{i} \bm{v}_{i} $$
\item That is we converge onto the set of eigenvectors with
eigenvalue 1
\item But if detailed balance is conserved then \(\bm{\pi}\)
satisfies this
\item There are some conditioned to ensure that there is only one
such eigenvector with eigenvalue 1
\begin{itemize}
\item We have to ensure that we can get from any state to any
other through some series of transitions
\item We have to prevent periodic behaviour (e.g. if our set of
states where from 1 to n and we could only move from state
\(i\) to state \(i-1\) or \(i+1\) then if we started on an odd
state we would always be on an odd state after an even
number of moves)
\item It is very easy to insure both conditions
\end{itemize}
\end{itemize}
\end{itemize}
\item Now the problem is to choose the transition probabilities to
satisfy detailed balance
\item This is easy and can be done in different ways
\item \textbf{Metropolis Algorithm}
\begin{itemize}
\item A very easy way to achieve detailed balance is that starting
from state \(i\) we choose a neighbouring state \(j\)
\begin{itemize}
\item We have to make sure the probability of choosing state \(i\)
when in state \(j\) is the same as the probability of choosing
state \(j\) when starting in state \(i\)
\end{itemize}
\item We now move to state \(j\) if
\begin{itemize}
\item \(\pi_{j} \geq \pi_{i}\)
\item or we move anyway with a probability \(\pi_{i}/\pi_{j}\)
\end{itemize}
\item It is a simple exercise to show that this satisfies detailed balance
\end{itemize}
\item \textbf{Metropolis-Hastings Algorithm}
\begin{itemize}
\item Sometimes it is difficult to insure that choosing state \(i\)
form state \(j\) is the same as choosing state \(j\) from state \(i\)
\begin{itemize}
\item As an example considering our states were non-negative
integers, we move from state \(i\) to state \(i-1\) and \(i+1\)
with probability \(\tfrac{1}{2}\), but we have a problem at
state 0 where we can't move to state \(i-1\)
\end{itemize}
\item We can modify the Metropolis algorithm.  Suppose \(p(i|j)\) is
the probability of choosing state \(i\) starting is state \(j\)
then let \(r = p(j|i) \, \pi_{i}/ (p(i|j)\,\pi_{j})\)
\begin{itemize}
\item we accept the move if \(r>1\)
\item or with a probability \(r\)
\item in practice we can just just draw a random number \(U\)
uniformly between 0 and 1 and accept the move if \(U<r\)
\item this is the same as the Metropolis algorithm if \(p(j|i) = p(i|j)\)
\end{itemize}
\end{itemize}
\item We can equally well apply this to (multi-dimensional) continuous
variables, \(\bm{\theta}\)
\begin{itemize}
\item If \(\bm{\theta}\) is our current set of parameters, we choose a
new set of parameter \(\bm{\theta}'\) with probability
\(p(\bm{\theta}'|\bm{\theta})\) and then accept this proposal if
\(r = (p(\bm{\theta}|\bm{\theta}') \, \pi(\bm{\theta}'))/
       (p(\bm{\theta}'|\bm{\theta}) \, \pi(\bm{\theta})) \geq 0\) or
with a probability \(r\)
\item We now choose \(p(\bm{\theta}'|\bm{\theta})\) so that with high
probability \(\bm{\theta}'\) is close to \(\bm{\theta}\), this
ensures that there is a reasonable high acceptance rate
\end{itemize}
\item For Bayes calculations then \(\pi(\bm{\theta})\) would be our
posterior \(f(\bm{\theta}|\mathcal{D})\)
\begin{itemize}
\item MCMC has the nice property that the update depends only on the
ratio
$$ \frac{\pi(\bm{\theta}')}{ \pi(\bm{\theta})} =
       \frac{f(\bm{\theta}'|\mathcal{D})}{f(\bm{\theta}|\mathcal{D})} 
       =
       \frac{f(\mathcal{D}|\bm{\theta}')\,f(\mathcal{D}|\bm{\theta}')}
       {f(\mathcal{D}|\bm{\theta})\,f(\mathcal{D}|\bm{\theta})} $$
\item That is, the normalising factor \(f(\mathcal{D})\) that appears
in Bayes' rule cancels
\item This is important because this is usually incomputable
(although we can use MCMC to estimate this)
\item There is also often another advantage of this ratio: if
we only change one variable at a time then frequently we can
compute the ratio much more efficiently than the full
likelihood and priors
\item When this happens then it pays to choose a single variable, \(\theta_{i}\), at a
time, chose a neighbour \(\theta'_{i}\) from a distribution
\(p(\theta'_{i}|\theta_{i})\) and decide whether to accept this update
\item This one-variable-at-a-time procedure goes by the name of
\emph{Gibbs' sampling}
\end{itemize}
\item \textbf{Convergence}
\begin{itemize}
\item A problem with MCMC is that your initial parameter, \(\bm{p}(0)\),
is different from \(\bm{\pi}\) so you have to wait
some considerable time before your samples are from posterior distribution
\item As we saw earlier the convergence rate is determined by the
second largest eigenvalue of the transition matrix \(\mat{M}\)
(we never explicit calculate \(\mat{M}\) so we don't know what
this eigenvalue is)
\begin{itemize}
\item If it is very close to 1 then your convergence can be slow
\end{itemize}
\item Furthermore if you posterior distribution is broad you need
lots of independent sample to accurately compute expectations
\item To obtain an independent sample we have to run our MCMC a long time
\item In practice you have to throw away some samples in a \emph{burn-in} period
\item You can then average over all the remaining samples you have (including
repetitions where you don't accept a move)
\item If you do this long enough then you should get samples that
cover values of \(\bm{\theta}\) with a high posterior probability
\item MCMC is slow because of this
\item There are a lot of clever tricks to speed up convergence and
decorrelation times
\end{itemize}
\item Often MCMC seems daunting because of the apparent difficulties
\begin{itemize}
\item But don't be put off
\item You only need to use the tricks for extremely complicated problems
\item Usually a simple implementation works fine
\item Modern computers are so fast that convergence isn't usually a problem
\begin{itemize}
\item Convergence can be a problem in pathological cases---these
occur when using MCMC to model some physical systems---but
for many Bayesian problems the Markov Chain is \emph{rapidly
mixing} meaning the convergence time is not excessively long
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Variational Techniques}
\label{sec:orgd3ac26a}
\begin{itemize}
\item This is a completely different approach to MCMC, but it tries to
solve the same problem, but uses a different strategy
\item This is again technically challenging and I don't expect you to
memorise the derivation
\item We saw in earlier lectures that we can cheat by seeking the parameters that maximise
the posterior (the, so called, MAP solution)
\item But this sacrifices all the probabilistic information
\item In variational techniques we try to approximate the posterior
with a simpler distribution
\item That is, we approximate the prior \(f(\bm{\theta}|\mathcal{D})\)
with a simpler distribution \(q(\bm{\theta}|\bm{w})\) where we get
choose \(\bm{w}\) to make \(q\) close to \(f\)
\item Usually we choose \(q\) to be a factorisable distribution
$$ q(\bm{\theta}|\bm{w}) = \prod_{i} q(\theta_{i}|w_{i}) $$
\item These techniques were first developed in the physics community
and so come with a strange language
\item In the \emph{variational approximations} we consider the \textbf{variational free
energy}
$$ \Phi(\bm{w}) = \int q(\bm{\theta}|\bm{w}) \,
     \logg{\frac{q(\bm{\theta}|\bm{w})}{f(\bm{\theta},\mathcal{D})}}
     \dd \bm{\theta} $$
\begin{itemize}
\item writing \(f(\bm{\theta},\mathcal{D}) =
       f(\bm{\theta}|\mathcal{D})\, f(\mathcal{D})\) we can rearrange the
variational free energy as
\begin{align*}
 \Phi(\bm{w}) &= \int q(\bm{\theta}|\bm{w}) \, \left(
 \logg{\frac{q(\bm{\theta}|\bm{w})}{f(\bm{\theta}|\mathcal{D})}} -
 \logg{f(\mathcal{D})} \right) \, \dd \bm{\theta} \\
 &= \mathrm{KL}\!\left(q(\bm{\theta}|\bm{w}) \big\|
 f(\bm{\theta}|\mathcal{D})\right)  - \logg{f(\mathcal{D})} 
 \end{align*}
\item where \(\mathrm{KL}\!\left(q(\bm{\theta}|\bm{w}) \big\|
        f(\bm{\theta}|\mathcal{D})\right)\) is the KL divergence that
 measures the "distance" between \(q(\bm{\theta}|\bm{w})\) and
\(f(\bm{\theta}|\mathcal{D})\)
\item we have previously shown that the KL-divergence is
non-negative and equal to zero if the two distributions are
the same
\item The term \(\logg{f(\mathcal{D})}\) is the \emph{marginal likelihood}
or \emph{evidence} and does not depend on the parameters \(\bm{w}\)
\item If we choose \(\bm{\theta}\) to minimise \(\Phi(\bm{w})\) then we
minimise the KL-divergence and force \(q(\bm{\theta}|\bm{w})\)
to approximate the posterior \(f(\bm{\theta}|\mathcal{D})\)
\item The variational free energy can also be written as
\begin{align*}
 \Phi(\bm{w}) &= -\int q(\bm{\theta}|\bm{w}) \,
 \logg{f(\bm{\theta},\mathcal{D})}   \dd \bm{\theta} +
 \int q(\bm{\theta}|\bm{w}) \,\logg{q(\bm{\theta}|\bm{w})}
 \dd \bm{\theta} \\
 &= - \left( \strut L_{q}(\bm{w}) + H_{q}(\bm{w}) \right)
\end{align*}
\item Minimising the variational free energy is equivalent to
maximising \(L_{q}(\bm{w}) + H_{q}(\bm{w})\)
\item \(L_{q}(\bm{w}) = \int q(\bm{\theta}|\bm{w}) \,
        \logg{f(\bm{\theta},\mathcal{D})}   \dd \bm{\theta}\)
 is the expected log-likelihood of the data, where we have
marginalised with respect to our approximate posterior
\(q(\bm{\theta}|\bm{w})\)
\item \(H_{q}(\bm{w}) = - \int q(\bm{\theta}|\bm{w})
       \,\logg{q(\bm{\theta}|\bm{w})} \dd \bm{\theta}\) is the entropy
of our approximate posterior (this measure the uncertainty in
\(\bm{\theta}\))
\item We thus balance maximising the likelihood of the data with
maximising the uncertainty in our approximate posterior
\item This means we fit the data, but guard against over fitting by
allowing there to be a non-zero probability wherever it does
not strongly contradict the data
\item In practice both \(L_{q}(\bm{w})\) and \(H_{q}(\bm{w})\) tend to be
quite easy to compute
\item We are left with an optimisation problem for the parameters,
\(\bm{w}\), but this is usually quite quick to compute
\end{itemize}
\end{itemize}

\section{Exercise}
\label{sec:org5b04c10}

\subsection{Mysterious Disease}
\label{sec:orgc9f4a4c}
\begin{itemize}
\item Recall in the lecture on probabilistic inference, we defined \(Z(t)\)
to be the number of people that catch a disease on day \(t\)
\item We assumed the growth rate is given by
$$ \Prob{Z(t+1)} = \mathrm{Poi}\!\left(Z(t+1)\middle|
     \frac{r_0}{3}\, (Z(t)+Z(t-1)+Z(t-2)) \right) $$
\begin{itemize}
\item that is people of contagious for three days
\end{itemize}
\item The observed number of new people with the disease, \(X(t)\), on
day \(t\) is 
$$ \Prob{X(t) = k} = \mathrm{Binom}(k|Z(t), p) = \binom{Z(t)}{k}
     p^k\,(1-p)^{Z(t)-k} $$
\item Here is \(X(t)\) for the first 30 days\\
0,0,0,0,1,1,2,1,0,0,3,10,19,34,44,93,117,221,376,633,\\
1021,1643,2701,4503,7414,12091,19999,33286,54612,90283\\
\item Estimate \(p\) and \(r_{0}\) from the data
\item (If you prefer simulate your own data and estimate your parameters)
\end{itemize}

\section{Answers}
\label{sec:org884fc13}

\subsection{Mysterious Disease}
\label{sec:org53da1fa}
\begin{itemize}
\item We need to decide on a prior
\begin{itemize}
\item A reasonable prior for \(p\) is a Beta distribution perhaps with
\(a=b=1\) (this is a uniform prior)
\item For \(r_{0}\) we could use a Gamma prior with \(a=2\) and \(b=1\),
this has a mean of \(r_{0} =2\) (it seems to be considerably
greater than 1)---we need to include \(r_{0}^{19}\e{-20\,r_{0}}}\) as our
prior (the normalisation is irrelevant)
\end{itemize}
\item Now we have to choose proposal distributions for the new values
of \(p\) and \(r_{0}\)
\begin{itemize}
\item You could use \(p' = p + U(-0.01,0.01)\) and \(r_{0}' = r_{0} +
       U(-0.1,0.1)\) where \(U(a,b)\) is a uniform random deviate
between \(a\) and \(b\)
\item This could have a problem in that \(p\) and \(r_{0}\) could take
illegal values (i.e. we should have \(0\leq r \leq 1\) and
\(r_{0} \geq0\)), but for my data this is unlikely to happen
\item We could choose \(p' \sim \mathrm{Beta}(10\,p, 10-10\,p)\) and
\(r_{0}' \sim r_{0}\,\mathrm{Gamma}(5,5) =
       \mathrm{Gamma}(5,5.r_{0})\), this ensures that in expectation
\(p'\) equals \(p\) and \(r_{0}'\) equals \(r_{0}\) with small
variations
\begin{itemize}
\item Beware that there are two common definitions of Gamma
distributions
\begin{align*}
\mathrm{Gamma}(z|,a,b) &= \frac{b^{a}}{\Gamma(a)}\,
z^{a-1}\,\e{-b\,z} \\
\mathrm{Gamma}^{*}(z|,\alpha,\beta) &= 
\frac{1}{\beta^{\alpha}\,\Gamma(\alpha)}
z^{\alpha-1}\,\e{-z/\beta}
\end{align*}
\item You have to check which of these your libraries use
\item In this case we would have Metropolis-Hastings to ensure we get
unbiased samples
\end{itemize}
\end{itemize}
\item The variables \(Z(t)\) are latent variables (they will vary for
each iteration) we can ignore them, but we could also estimate
their mean as they tell us the actual number of cases of people with
the virus
\begin{itemize}
\item My data was generated with \(p=0.1\) and \(r_{0}=2.5\)
\end{itemize}
\end{itemize}
\end{document}
