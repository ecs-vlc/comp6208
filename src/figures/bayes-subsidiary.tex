% Created 2021-01-28 Thu 17:26
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[a4paper,margin=20mm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{bm}
\usepackage{minted}
\usemintedstyle{emacs}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beraserif}
\usepackage[scaled]{berasans}
\usepackage[scaled]{beramono}
\newcommand{\tr}{\textsf{T}}
\newcommand{\grad}{\bm{\nabla}}
\newcommand{\av}[2][]{\mathbb{E}_{#1\!}\left[ #2 \right]}
\newcommand{\Prob}[2][]{\mathbb{P}_{#1\!}\left[ #2 \right]}
\newcommand{\logg}[1]{\log\!\left( #1 \right)}
\newcommand{\pred}[1]{\left\llbracket { \small #1} \right\rrbracket}
\newcommand{\e}[1]{{\rm e}^{#1}}
\newcommand{\dd}{\mathrm{d}}
\DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{n}
\newcommand{\normal}[2]{\mathcal{N}\!\left(#1 \big| #2 \right)}
\newcounter{eqCounter}
\setcounter{eqCounter}{0}
\newcommand{\explanation}{\setcounter{eqCounter}{0}\renewcommand{\labelenumi}{(\arabic{enumi})}}
\newcommand{\eq}[1][=]{\stepcounter{eqCounter}\stackrel{\text{\tiny(\arabic{eqCounter})}}{#1}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\Dist}[2][Binom]{\mathrm{#1}\left( \strut {#2} \right)}
\author{Adam Prügel-Bennett}
\date{\today}
\title{Advanced Machine Learning Subsidary Notes\\\medskip
\large Lecture 16: Bayesian Inference}
\hypersetup{
 pdfauthor={Adam Prügel-Bennett},
 pdftitle={Advanced Machine Learning Subsidary Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section{Keywords}
\label{sec:org8833a8b}
\begin{itemize}
\item Bayes, Conjugate Priors, Uninformative Priors
\end{itemize}

\section{Main Points}
\label{sec:org4941a69}

** Bayesian Statistics
\begin{itemize}
\item Background
\begin{itemize}
\item Bayesian statistics is an approach to making statistical inference
\item It was first proposed by the Rev. Thomas Bayes but published after his death in 1761
\item It was championed by the great French polymath Pierre-Simon Laplace
\item It fell out of favour at the beginning of the \(20^{th}\) Century due to philosophical prejudice
\begin{itemize}
\item It was deemed to be unscientific because it requires specifying
a prior probability which seemed objective
\end{itemize}
\item It was championed from the 1950's by Ed Jaynes who eventually
convinced most people that Bayesian inference is consistent
\item Perhaps more importantly with the raise of computers it was
possible to demonstrate that Bayesian methods work
\end{itemize}
\item Bayes' Rule
\begin{itemize}
\item Bayes rule follows from a simple identity
$$ \Prob{h_i|\mathcal{D}} =
        \frac{\Prob{\mathcal{D}|h_i}\,\Prob{h_i}}{\Prob{\mathcal{D}}} $$
\begin{itemize}
\item \(\Prob{h_i|\mathcal{D}}\) is the \emph{posterior} probability of
hypothesis \(h_i\) given data \(\mathcal{D}\)
\item \(\Prob{\mathcal{D}|h_i}\) is the \emph{likelihood} of the data
given the hypothesis \(h_i\)
\item \(\Prob{h_i}\) is our \emph{prior} probability (our belief in
hypothesis \(h_i\) before seeing the data
\item \(\Prob{\mathcal{D}}\) is a normalisation terms I will call the
\emph{evidence} and is just equal to
$$ \Prob{\mathcal{D}} = \sum_{i=1}^n\Prob{h_i,\mathcal{D}}
          = \sum_{i=1}^n \Prob{\mathcal{D}|h_i} \,\Prob{h_i} $$
\end{itemize}
\end{itemize}
\item Although this is just a mathematical identity, to use it in
inference requires specifying a prior belief
\begin{itemize}
\item I argued from the first lecture that for machine learning to
work we need to make assumptions about the data
\item In Bayesian approach these assumptions our built into our
likelihood and prior
\end{itemize}
\item A nice property of Bayesian inference is that it turns the
problem from an inverse problem (computing the posterior
\(\Prob{h_i|\mathcal{D}}\)) into a forward problem of computing
the likelihood \(\Prob{\mathcal{D}|h_i}\)
\begin{itemize}
\item This solves problems of missing data (we only need to
calculate the probability of the data we see)
\item It is also much more straightforward to model the forward
process (from physics) rather than the inverse process
\item That said, it can still be technically very challenging
\end{itemize}
\item Bayes' rule has the same form whether we are working with
discrete variables (where we have probability masses) or
continuous variables (where we have probability densities)
\end{itemize}

\subsection{Conjugate Priors}
\label{sec:org3ab7daa}
\begin{itemize}
\item Technically performing Bayesian inference can be difficult
because the posterior can be very ugly to work with
\item But for a few of the classic likelihood functions there exist
\emph{conjugate prior distributions} such that the posterior is in the
same class of distributions as the prior
\begin{itemize}
\item Suppose that the likelihood of observing data \(\bm{X}\) given
some parameters \(\bm{\theta}\) is \(p(\bm{X}|\bm{\theta})\) (this may be
a probability mass or density depending on whether \(\bm{X}\) is
continuous or not)
\item We want to infer \(\bm{\theta}\) which we do through the
posterior \(p(\bm{\theta}|\bm{X})\)
\item If we are lucky there is a family of distributions,
\(\mathrm{Conj}(\bm{\theta}|\bm{\phi})\) parameterised by
\(\bm{\phi}\), that describe the parameters of the likelihood
\(\bm{\theta}\) and satisfy
$$ \mathrm{Conj}(\bm{\theta}|\bm{\phi}') \propto
       p(\bm{X}|\bm{\theta})\, \mathrm{Conj}(\bm{\theta}|\bm{\phi}') $$
\item Thus if we start with a prior
\(\mathrm{Conj}(\bm{\theta}|\bm{\phi}_0)\) then after observing
data \(\bm{X}_1\) we would end up with a posterior
\(\mathrm{Conj}(\bm{\theta}|\bm{\phi}_1)\) where \(\bm{\phi}_1\)
depends on observation \(\bm{X}_1\) and \(\bm{\phi}_0\)
\item We can repeat this
\begin{itemize}
\item When we make a second observation \(\bm{X}2\) our posterior now
becomes our prior (we have updated our beliefs) so now we
obtain a new posterior
$$  \mathrm{Conj}(\bm{\theta}|\bm{\phi}_2) \propto
         p(\bm{X}_2|\bm{\theta})\, \mathrm{Conj}(\bm{\theta}|\bm{\phi}_1) $$
\end{itemize}
\end{itemize}
\item As an example let us consider a number of Bernoulli trials (we
assume independent observations \(X_i\in\{0,1}\) occurring with
probability \(p\)
\begin{itemize}
\item But we don't know \(p\)
\item We assume our prior distribution is a beta distribution
\(\mathrm{Beta}(p|a_0,b_0) \propto p^{a_0-1}\,(1-p)^{b_0-1}\)
\item Our likelihood is \(\Prob{X|p}=p^X\,(1-p)^{1-X}\)
\item Using Bayes' rule our posterior is
$$ f(p|X) \propto p^X\,(1-p)^{1-X} \, p^{a_0-1}\,(1-p)^{b_0-1}
       = p^{a_0+X-1}\,(1-p)^{b_0+1-X-1} $$
\item But this has the same functional form as a beta-distribution
\item It will be a beta-distribution as it has to be normalised
(posteriors are always normalised)
\item Thus \(f(p|X) = \mathrm{Beta}(p|a_1,b_1)\) where \(a_1=a_0+X\) and
\(b_1=b_0+1-X\)
\end{itemize}
\item The Poisson distribution is defined as
$$ \mathrm{Pois}[N|\mu] = \frac{\mu^N\,\e{-\mu}}{N!} $$
\begin{itemize}
\item It describes the probability of observing \(N\) events where each
event is assumed independent and the expected number of events
is \(\mu\)
\item Suppose we have made observations \(N_1\), \(N_2\), etc. and we are
trying to infer \(\mu\)
\item The Poisson distribution has a conjugate distribution which is
the gamma distribution
$$ \mathrm{Gamma}(\mu|a,b) =
       \frac{b^a\,\mu^{a-1}\,\e{-b\,\mu}}{\Gamma(a)} $$
\item Assuming a prior \(\mathrm{Gamma}(\mu|a_0,b_0)\) and assume we
make an observation \(N_1\) then the posterior is given by
$$ f(\mu|N_1}) \propto \mu^{N_1}\,\e{-\mu} \,
       \mu^{a_0-1}\,\e{-b_0\,\mu} = \mu^{a_0+N_1-1}\,\e{-(b_0+1)\,\mu} $$
\item Thus \(f(\mu|N_1})=\mathrm{Gamma}(\mu|a_1,b_1)\) where
\begin{itemize}
\item \(a_1 = a_0 +N_1\)
\item \(b_1 = b_0 +1\)
\end{itemize}
\end{itemize}
\item Conjugate priors are the exception rather than the rule but they
do occur for some classic distributions.  E.g.

\begin{center}
\begin{tabular}{ll}
\textbf{Likelihood} & \textbf{Prior}\\
\hline
Binomial/Bernoulli & Beta\\
Poisson & Gamma\\
Multinomial & Dirchlet\\
Univariate Normal & Gamma-Normal\\
Multivariate Normal & Wishart\\
\hline
\end{tabular}
\end{center}
\end{itemize}

\subsection{Uninformative Priors}
\label{sec:org88538ba}
\begin{itemize}
\item What should we do if we have no prior information
\item This disturbed statisticians who felt that there might not be a
subjective answer to this
\item However, Ed Jaynes argued that there is a unique answer that we
get by requiring invariance
\item Scale parameters
\begin{itemize}
\item Often we are trying to infer scale parameters
\item These would include the rate in the Poisson distribution or the
standard deviation in a normal distribution
\item They are non-negative numbers that determine the scale
(i.e. what units we should measure in)
\item For most problems we have some idea about this (otherwise if
would be difficult to do the measurement)
\item But we may still not no the order of magnitude of what we are
measure in
\item What should we use as a prior?
\item The answer to this was given by Harold Jeffreys and is known as
the \emph{Jeffreys' prior}
\item Let's see the argument
\item If we have not idea about what scale an observable \(x\) takes
then we should expect there to be equal probability to be in
the intervals \([A,B]\) or the interval \([A/c,B/c]\)
\item This may seem strange as these two intervals are different
lengths (but non-overlapping if \(c\neq1\)) but if this wasn't
the case we would know something about which scale to use
\item If our prior for \(x\) is \(p(x)\) then we require
$$ \int_A^B p(x)\,\dd x = \int_{A/c}^{B/c} p(x)\,\dd x $$
\item Now we can make a change of variables
\item \(y=c\,x\) so this last integral becomes
$$ \int_{A/c}^{B/c} p(x)\,\dd x = \int_A^B \frac{1}{c} \,
       p\!\left(\frac{y}{c}\right) \dd y$$
\item But for this to equal the first integral for any interval
\([A,B]\) we require
$$ p(x) \propto \frac{1}{x} $$
\item We note that if \(p(x)=1/x\) then
$$ p\!\left(\frac{x}{c}\right)  = \frac{c}{x} = c \, p(x) $$
as required
\item The strange thing about this distribution is it in improper in
that 
$$ \int_0^\infty \frac{1}{x} = \infty $$
\item Strangely a lot of uninformative priors turn out to be improper
\item However, this doesn't seem to matter in Bayesian inference:
after making some observations we end up with a proper prior
\end{itemize}
\item \textbf{Benford's Law}
\begin{itemize}
\item There is a strange consequence of Jeffreys prior which was
first speculated upon in 1881 when Simon Newcomb noticed that
logarithm tables were much more used for numbers beginning with
1 rather 2 and 2 rather than 3, etc.  Frank Benford in 1938
looked at the occurrence of numbers in data set and found that
the digit of first significant figure was more likely to be 1
than 2 and more likely to be 2 than 3.
\item If we assume these numbers are measured in an arbitrary scale
and the occurrence of these numbers followed Jeffreys prior
then the probability that real number between 1 and 10 actually
took values between \(n\) and \(n+1\) is given by
$$ \frac{\int_n^{n+1} \frac{1}{x} \dd x}{\int_1^{10} \frac{1}{x}
       \dd x} = \frac{\log(n+1) - \log(n)}{\log(10)} =
       \logg{\frac{n+1}{n}} $$
\item Note that we would get the same result if the number was
between 10 and 100 and we asked the probability that it was in
the range \(10\,n\) to \(10\,(n+1)\)
\item But this means the digit of the most significant figure will be \(n\)
\item So numbers beginning with 1 occur more often than numbers
beginning with 2, and numbers beginning with 2 occur more often
than those beginning with 3, etc.
\item This is weird but true (for most naturally occurring
non-negative numbers)
\end{itemize}
\end{itemize}


\section{Exercises}
\label{sec:orga0989b1}

\subsection{Throwing Dice}
\label{sec:org994d206}
\begin{itemize}
\item There are three dice on the table.  Two of them are normal dice,
while the other dice has 6 on two faces and 1, 2, 3 and 5 on the other
face.  A dice is chosen at random and is thrown 10 times.  On three
occasions the top face is a 6.  What is the probability that the dice
chosen is the dishonest dice?
\item See answers below
\end{itemize}


\section{Experiments}
\label{sec:org64e845e}

\subsection{Benford's Law}
\label{sec:org2df9599}
\begin{itemize}
\item Benford's law is so wacky that to convince yourself it is true
you really need to test it out
\item Find a set of data with features that are clearly scale
parameters and have a go (you need enough data points to be
convincing)
\begin{itemize}
\item if you numbers are positive and could take any value then they
are likely to be scale parameter
\end{itemize}
\end{itemize}

\section{Answers}
\label{sec:org2b809dd}

\subsection{Throwing Dice}
\label{sec:orgca29903}
\begin{itemize}
\item We use Bayes' rule
$$ P(\text{dishonest}|\text{data}) = \frac{P(\text{data}|\text{dishonest})
      \,P(\text{dishonest} )}
      {P(\text{data}|\text{dishonest})\,P(\text{dishonest} ) +
      P(\text{data}|\text{honest})\,P(\text{honest})} $$
\item Now \(P(\text{dishonest} )=1/3\) and \(P(\text{honest})=2/3\)
\item The probability of \(k\) success out of \(n\) trials with a success
probability \(p\) is given by the binomial distribution
\end{itemize}
$$\mathrm{Binom}(k|n,p) = \binom{n}{k} p^k\,(1-p)^{n-k} $$
\begin{itemize}
\item The probability of 3 out of 10 rolls being a 6 is given by
\begin{align*}
P(\text{data}|\text{dishonest}) &=
\mathrm{Binom}(3|10,\tfrac{1}{3}) = \binom{3}{10}
\left(\frac{1}{3}\right)^3  \left(\frac{2}{3}\right)^7\\ \\
P(\text{data}|\text{honest}) &=
\mathrm{Binom}(3|10,\tfrac{1}{6}) =  \binom{3}{10}
\left(\frac{1}{6}\right)^3  \left(\frac{5}{6}\right)^7
\end{align*}
\item Thus
\begin{align*}
P(\text{dishonest}|\text{data}) &= \frac{1}{1 +
  \frac{P(\text{data}|\text{honest})\,P(\text{honest})}
  {P(\text{data}|\text{dishonest})\,P(\text{dishonest})}} \\ \\
&= \frac{1}{1 + 2 \left(\frac{1}{2}\right)^3
  \left(\frac{5}{4}\right)^7} = 0.29549 \\
\end{align*}
\end{itemize}
\end{document}
